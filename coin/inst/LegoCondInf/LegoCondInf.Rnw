\documentclass{article}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage[nolists]{endfloat}

\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Robject}[1]{\texttt{#1}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rcmd}[1]{\texttt{#1}}
\newcommand{\Roperator}[1]{\texttt{#1}}
\newcommand{\Rarg}[1]{\texttt{#1}}
\newcommand{\Rlevel}[1]{\texttt{#1}}

\newcommand{\RR}{\textsf{R}} 
\renewcommand{\S}{\textsf{S}}

\newcommand{\R}{\mathbb{R} }
\newcommand{\Prob}{\mathbb{P} }
\newcommand{\N}{\mathbb{N} }
\newcommand{\C}{\mathbb{C} }
\newcommand{\V}{\mathbb{V}} %% cal{\mbox{\textnormal{Var}}} }
\newcommand{\E}{\mathbb{E}} %%mathcal{\mbox{\textnormal{E}}} }
\newcommand{\Var}{\mathbb{V}} %%mathcal{\mbox{\textnormal{Var}}} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\LS}{\mathcal{L}_n}
\newcommand{\TS}{\mathcal{T}_n}
\newcommand{\LSc}{\mathcal{L}_{\text{comb},n}}
\newcommand{\LSbc}{\mathcal{L}^*_{\text{comb},n}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\yn}{y_{\text{new}}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\sX}{\mathcal{X}}
\newcommand{\sY}{\mathcal{Y}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\xn}{\mathbf{x}_{\text{new}}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\ws}{\mathbf{w}_\cdot}
\renewcommand{\t}{\mathbf{t}}
\newcommand{\M}{\mathbf{M}}
\renewcommand{\vec}{\text{vec}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\K}{\mathbf{K}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\cellx}{\pi_n[\x]}
\newcommand{\partn}{\pi_n(\mathcal{L}_n)}
\newcommand{\err}{\text{Err}}
\newcommand{\ea}{\widehat{\text{Err}}^{(a)}}
\newcommand{\ecv}{\widehat{\text{Err}}^{(cv1)}}
\newcommand{\ecvten}{\widehat{\text{Err}}^{(cv10)}}
\newcommand{\eone}{\widehat{\text{Err}}^{(1)}}
\newcommand{\eplus}{\widehat{\text{Err}}^{(.632+)}}
\newcommand{\eoob}{\widehat{\text{Err}}^{(oob)}}

\SweaveOpts{engine=R,eps=FALSE}

\hypersetup{%
  pdftitle = {A Lego-System for Conditional Inference},
  pdfsubject = {Manuscript},
  pdfauthor = {Torsten Hothorn, Kurt Hornik,
               Mark van de Wiel and Achim Zeileis},
%% change colorlinks to false for pretty printing
  colorlinks = {true},
  linkcolor = {blue},
  citecolor = {blue},
  urlcolor = {red},
  hyperindex = {true},
  linktocpage = {true},
}

\title{A Lego-System for Conditional Inference}

\author{Torsten Hothorn$^1$, Kurt Hornik$^2$, \\ 
        Mark van de Wiel$^3$ and Achim Zeileis$^2$}
\date{}

\begin{document}

\setkeys{Gin}{width=\textheight}


\maketitle

\noindent$^1$Institut f\"ur Medizininformatik, Biometrie und Epidemiologie\\
     Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg\\
     Waldstra{\ss}e 6, D-91054 Erlangen, Germany \\
     \texttt{Torsten.Hothorn@R-project.org}
\newline

\noindent$^2$Department f\"ur Statistik und Mathematik,
             Wirtschaftsuniversit\"at Wien \\
       Augasse 2-6, A-1090 Wien, Austria \\
       \texttt{Kurt.Hornik@R-project.org} \\
       \texttt{Achim.Zeileis@R-project.org}
\newline

\noindent$^3$ Department of Mathematics and Computer Science \\
              Eindhoven University of Technology \\
              HG 9.25, P.O. Box 513 \\
              5600 MB Eindhoven, The Netherlands \\
              \texttt{markvdw@win.tue.nl}
\newline

<<setup, echo = FALSE, results = hide>>=
options(width = 65)
require("coin")
require("multcomp")
set.seed(290875)
data("alzheimer", package = "coin")
data("photocar", package = "coin")
data("mercuryfish", package = "coin")
### get rid of the NAMESPACE
load(file.path(.find.package("coin"), "R", "all.rda"))
@

\thispagestyle{empty}

\begin{abstract}
Conditioning on the observed data is an important and flexible 
design principle for statistical test procedures. Although generally
applicable, most text book and software implementations are limited to
the treatment of special cases. A new theoretical framework for permutation
tests opens up the way to a unified and generalized view. We argue that the
transfer of such a theory to practical data analysis has important
implications in many applications and requires a software implementation 
that enables the
data analyst to compute on the theoretical concepts as closely as possible.
We re-analyze data where non-standard inference procedures are
required utilizing the \Rpackage{coin} add-on package in the \textsf{R}
system for statistical computing and show what one can gain from going
beyond pre-packaged test procedures.
\end{abstract}

\noindent
KEY WORDS: Permutation tests; Multiple testing; Independence; Software.

\newpage

\section{Introduction}

The distribution of a test statistic under the circumstances of a certain
null hypothesis clearly depends on the unknown distribution of the data and
thus is unknown as well.
Two concepts are commonly applied to dispose of this dependency.
Unconditional tests impose assumptions on the distribution of the data such
that the null distribution can be derived analytically. In contrast, 
conditional tests
replace the unknown null distribution by the conditional null distribution,
i.e. the distribution of the test statistic given the observed data. The
latter approach is known as \textit{permutation testing} and was developed
by R. A. Fisher in the 1930s \citep{Fisher1935}. 

The pros and cons of both approaches have been discussed in extensio
\citep[e.g. by][]{why-permut:1998,pros-and-c:2000,Shuster2005} and we
refrain from stepping into this mostly philosophical debate, noting that most
conditional test procedures are asymptotically equivalent to their
unconditional counterparts anyway.

For the construction of permutation tests it is common exercise to `recycle'
test statistics well known from the unconditional world, such as linear rank
statistics, ANOVA $F$ statistics or $\chi^2$ statistics for
contingency tables, and to replace the unconditional null distribution with
the conditional distribution of the test statistic under the null
hypothesis \citep{Edgington1987,Good2000,Pesarin2001,Ernst2004}. 
Such a classification into inference problems (categorical data
analysis, multivariate analysis, $K$-sample location problems, correlation
etc.) each being associated with a `natural' form of the test statistic
obstructs our view on the common foundations of all permutation tests. 
Theoretical advances in the last decade
\citep{StrasserWeber1999,JanssenPauls2003} helped us to understand the strong
connections between the `classical' permutation tests and open up the way to
a simple construction principle for test procedures in new and
challenging inference problems.

Especially attractive for this purpose is the theoretical framework for
permutation tests developed by \cite{StrasserWeber1999}. This unifying
theory is based on a flexible form of multivariate linear statistics for the
general independence problem those conditional expectation and covariance is
trackable. The classical procedures, such as a permutation $t$ test, are part
of this framework and, even more interesting, new test procedures can be
embedded into the same theory. 

It is one mission, if not \textit{the} mission, of statistical computing to
transform new theoretical developments into flexible software tools for the
data analyst. Currently, our toolbox consists of rather inflexible spanners,
such as \Rcmd{wilcox.test} for the Wilcoxon-Mann-Whitney test or
\Rcmd{mantelhaen.test} for the Cochran-Mantel-Haenszel chi-squared test in
\S{} languages. The implementation of permutation tests with user interfaces
designed to deal with very special cases only \citep[see the Tables in][for
an overview on procedures implemented in StatXact, LogXact, Strata, SAS and
Testimate]{Oster2002,Oster2003} leads to the classical `cook book'
statistics. Such cook books, and thus software implementations, 
typically teach recipes and hide the concepts which are necessary to go beyond 
the implemented procedures when the data analyst is faced with non-standard
inference problems or wants to perform a test not supported by the preferred
software package.

With this work, we add an adjustable spanner to the statisticians toolbox 
which helps to address both the common as well as new or unusual inference 
problems with the appropriate conditional test procedures. The
\Rpackage{coin} add-on package to the \RR{} system for statistical computing
\citep{Rcore2005} essentially is a software instance of the Strasser-Weber
framework for the generalized independence problem which allows for
computations directly on the theory those main concepts are sketched
in Section~\ref{CI}. In the main part of this paper we show how one can
build permutation tests `on the fly' by plugging together Lego bricks for
the multivariate linear statistic, the test statistic and the conditional
null distribution. 


\section{Conditional Inference \label{CI}}

To fix ideas we assume that we are provided with observations
$(\Y_i, \X_i)$ for $i = 1, \dots, n$.
The variables $\Y$ and $\X$ from sample spaces $\mathcal{Y}$ and
$\mathcal{X}$ may
be measured at arbitrary scales and may be multivariate as well. 

We are interested in testing the null hypothesis of independence of $\Y$ and $\X$
\begin{eqnarray*}
H_0: D(\Y | \X) = D(\Y)
\end{eqnarray*}
against arbitrary alternatives. \cite{StrasserWeber1999} suggest to derive
scalar test statistics for testing $H_0$ from multivariate linear statistics
of the form 
\begin{eqnarray} \label{linstat}
\T = \vec\left(\sum_{i = 1}^n g(\X_i) h(\Y_i, (\Y_1, \dots, \Y_n))^\top\right)
\in \R^{pq}.
\end{eqnarray}
Here, $g: \mathcal{X} \rightarrow \R^{p}$ is a transformation of
the $\X$ measurements and the \emph{influence function}
$h: \mathcal{Y} \times \mathcal{Y}^n \rightarrow
\R^q$ depends on the responses $(\Y_1, \dots, \Y_n)$ in a permutation
symmetric way. We will give specific examples how to choose $g$ and $h$
for specific inference problems later on.

The distribution of $\T$  depends on the joint distribution of $\Y$ and $\X$, 
which is unknown under almost all practical circumstances. 
At least under the null hypothesis one can dispose of this 
dependency by fixing $\X_1, \dots, \X_n$ and conditioning on all possible 
permutations $S$ of the responses $\Y_1, \dots, \Y_n$. 

The conditional expectation $\mu \in \R^{pq}$ and covariance
$\Sigma \in \R^{pq \times pq}$ of $\T$ under $H_0$ given
all permutations $\sigma \in S$ of the responses are derived by
\cite{StrasserWeber1999}:
\begin{eqnarray}
\mu = \E(\T | S) & = & \vec \left( \left( \sum_{i = 1}^n g(\X_i) \right)
\E(h | S)^\top \right) \nonumber \\
\Sigma = \V(\T | S) & = &
    \frac{n}{n - 1}  \V(h | S) \otimes
        \left(\sum_i g(\X_i) \otimes  g(\X_i)^\top \right)
\label{expectcovar}
\\
& - & \frac{1}{n - 1}  \V(h | S)  \otimes \left(
        \sum_i g(\X_i) \right) \otimes \left( \sum_i g(\X_i)\right)^\top
\nonumber
\end{eqnarray}
where $\otimes$ denote the Kronecker product. The conditional expectation of the
influence function is
\begin{eqnarray*}
\E(h | S) = n^{-1} \sum_i h(\Y_i, (\Y_1, \dots, \Y_n)) \in
\R^q
\end{eqnarray*}
with corresponding $q \times q$ covariance matrix $\V(h | S)$ given by
\begin{eqnarray*}
n^{-1} \sum_i \left(h(\Y_i, (\Y_1, \dots, \Y_n)) - \E(h | S)
\right) \left(h(\Y_i, (\Y_1, \dots, \Y_n)) - \E(h | S)\right)^\top.
\end{eqnarray*}

The key step for the construction of test statistics from the multivariate
linear statistic $\T$ is its standardization utilizing the 
the conditional expectation $\mu$ and covariance matrix $\Sigma$. 
Univariate test statistics~$c$ mapping an observed linear
statistic $\mathbf{t} \in \R^{pq}$ 
into the real line can be of arbitrary form.  An obvious choice is
the maximum of the absolute values of the standardized linear statistic
\begin{eqnarray*}
c_\text{max}(\mathbf{t}, \mu, \Sigma)  = \max \left| \frac{\mathbf{t} -
\mu}{\text{diag}(\Sigma)^{1/2}} \right|
\end{eqnarray*}
utilizing the conditional expectation $\mu$ and covariance matrix
$\Sigma$. A prominent alternative are quadratic forms 
$c_\text{quad}(\mathbf{t}, \mu, \Sigma)  =
(\mathbf{t} - \mu) \Sigma^+ (\mathbf{t} - \mu)^\top$ involving 
the Moore-Penrose inverse $\Sigma^+$ of $\Sigma$.

%%The definition of one- and two-sided $p$-values used for the computations in
%%the \Rpackage{coin} package is
%%\begin{eqnarray*}
%%& & P(c(\T, \mu, \Sigma)\le c(\mathbf{t}, \mu, \Sigma)) \quad \text{(less)} \\  
%%& & P(c(\T, \mu, \Sigma) \ge c(\mathbf{t}, \mu, \Sigma)) \quad \text{(greater)}\\
%%& & P(|c(\T, \mu, \Sigma)| \le |c(\mathbf{t}, \mu, \Sigma)|) \quad
%%\text{(two-sided).}
%%\end{eqnarray*}
%%Note that for quadratic forms only two-sided $p$-values are available 
%%and that in the one-sided case maximum type test statistics are replaced by
%%\begin{eqnarray*}
%%\min \left( \frac{\mathbf{t} - \mu}{\text{diag}(\Sigma)^{1/2}} \right)
%%    \quad \text{(less) and } 
%%\max \left( \frac{\mathbf{t} - \mu}{\text{diag}(\Sigma)^{1/2}} \right)
%%    \quad \text{(greater).}
%%\end{eqnarray*}

The conditional distribution $\Prob(c(\T, \mu, \Sigma) \le z | S)$
is the number of permutations $\sigma \in S$ of the data 
with corresponding test statistic less than $z$ divided by the total number
of permutations in $S$. For some special forms of the
multivariate linear statistic the exact distribution of some 
test statistics is trackable for small to moderate sample sizes.
%%For two-sample problems, the shift-algorithm by \cite{axact-dist:1986} 
%%and \cite{exakte-ver:1987} and the split-up algorithm by 
%%\cite{vdWiel2001} are implemented as part of the package.

The conditional distribution can be approximated by the limit distribution
under all circumstances. \cite{StrasserWeber1999} proved (Theorem 2.3) that the   
conditional distribution of linear statistics $\T$ with conditional    
expectation $\mu$ and covariance $\Sigma$ tends to a multivariate normal
distribution with parameters $\mu$ and $\Sigma$ as $n \rightarrow
\infty$. Thus, the asymptotic conditional distribution of test statistics of
the form $c_\text{max}$ is normal and
can be computed directly in the univariate case ($pq = 1$)
or itself being approximated by means of quasi-randomized Monte-Carlo  
procedures in the multivariate setting \citep{numerical-:1992}. For
quadratic forms
$c_\text{quad}$ which follow a $\chi^2$ distribution with degrees of freedom
given by the rank of $\Sigma$ \citep[e.g. Theorem 6.20, ][]{Rasch1995}, exact
probabilities can be computed efficiently.

Conditional Monte-Carlo procedures can also be used to 
approximate the exact distribution up to any desired accuracy by evaluating
the test statistic for a random sample from the set all permutations $S$.
It is important to note that the presence of a grouping of the observations
into blocks, only permutations within blocks are eligible and that the
conditional expectation and covariance matrix needs to be computed
separately for each block.


\section{Playing Lego with \Rpackage{coin}}

The \Rpackage{coin} package implements software infrastructure for the main
components of the theoretical framework sketched above, namely the linear
statistic $\T$ with user-defined transformations $g$ and influence functions
$h$, functions for the computation of the conditional expectation $\mu$ and
covariance matrix $\Sigma$ as in (\ref{expectcovar}) and utilities for the
computation of the conditional distribution of $c_\text{max}$ or
$c_\text{quad}$ test statistics. Thus, the flexibility of the theoretical
framework is translated and preserved in a software instance which enables
the data analyst to benefit from this conceptually simple methodology in
every day's data analysis.

\paragraph{Smoking and Alzheimer's Disease.}

<<alzheimer-demographics, echo = FALSE>>=
total <- sum(alzheimer)
stopifnot(total == 538)
male <- sum(alzheimer[,,"Male"])
stopifnot(male == 200)
female <- sum(alzheimer[,,"Female"])
stopifnot(female == 338)
disease <- colSums(margin.table(alzheimer, margin = c(1,2)))
smoked <- sum(rowSums(margin.table(alzheimer, margin = c(1,2)))[-1])
### there is a discrepancy between Table 1 (32% smokers of 117 women 
### suffering from other diagnoses) and Table 4 (63% non-smokers).
### We used the data as given in Table 4.
@

\cite{SalibHillier1997} report results of a case-control study on
Alzheimer's disease and smoking behavior of 
$\Sexpr{disease["Alzheimer's"]}$
patients suffering from Alzheimer's disease and 
$\Sexpr{disease[names(disease) != "Alzheimer's"]}$
controls. The data shown in Table~\ref{alzheimertab} have been 
re-constructed from Table~4 in \cite{SalibHillier1997}. 
The authors conclude that `cigarette smoking is less frequent in 
men with Alzheimer's disease'. 

\begin{table}[h]
\begin{center}
\caption{\Robject{alzheimer} data: Smoking and Alzheimer's disease. \label{alzheimertab}}
\begin{tabular}{lrrrr} \hline \hline
 & \multicolumn{4}{c}{No. of cigarettes daily} \\
 & None & <10 & 10--20 & >20 \\ \hline
\textit{Female} & & & & \\
<<alzheimer-tab, echo = FALSE, results = tex>>=
x <- t(alzheimer[,,"Female"])
lines <- paste(paste(dimnames(x)$disease, " & "), 
               paste(apply(x, 1, function(l) paste(l, collapse = " & ")), "\\\\"))
for (i in 1:length(lines)) cat(lines[i], "\n")
@

& & & & \\
\textit{Male} & & & & \\
<<alzheimer-tab, echo = FALSE, results = tex>>=
x <- t(alzheimer[,,"Male"])
lines <- paste(paste(dimnames(x)$disease, " & "), 
               paste(apply(x, 1, function(l) paste(l, collapse = " & ")), "\\\\"))
for (i in 1:length(lines)) cat(lines[i], "\n")
@
\hline
\end{tabular}
\end{center}
\end{table}

Ignoring the ordinal structure of the smoking behavior, the null hypothesis
of independence between smoking and disease status treating gender as a
block factor with a $c_\text{quad}$-type test statistic, i.e. the conditional
version of the Cochran-Mantel-Haenszel test
<<alzheimer-mantelhaen, echo = TRUE>>=
data("alzheimer", package = "coin")
it_alz <- independence_test(alzheimer, teststat = "quadtype")
it_alz
@
suggests that there is a clear deviation from independence. 
By default, the influence function $h$ and the transformation $g$ 
are dummy codings of the disease status $\Y$ and the smoking behavior $\X$, 
i.e. $h(\Y_i, (\Y_1, \dots, \Y_n)) = (1, 0, 0)$ 
and $g(\X_i) = (1, 0, 0 ,0)$ for a non-smoking Alzheimer patient. 
Consequently, the linear multivariate statistic $\T$ based on $g$ and $h$ 
is the contingency table of both variables 
<<alzheimer-statistic, echo = TRUE>>=
statistic(it_alz, type = "linear")
@
with conditional expectation \Rcmd{expectation(it\_alz)} and conditional
covariance \Rcmd{covariance(it\_alz)} which are available for standardizing
the contingency table $\T$. The conditional distribution is approximated by
its limiting $\chi^2$ distribution by default. This leads to exactly the
same $p$-value as the unconditional test 
<<alzheimer-mantelhaen.test, echo = TRUE>>= 
pvalue(it_alz) 
mantelhaen.test(alzheimer)$p.value
@
The form of the deviation from independence is of special interest,
however, a chi-squared statistic is not particular useful for this purpose.
Instead, we define the test statistic as the maximum of the 
standardized contingency table via
<<alzheimer-max, echo = TRUE>>=
it_alzmax <- independence_test(alzheimer, teststat = "maxtype")
it_alzmax
@
which leads to virtually the same $p$-value. The standardized contingency
table sheds some light on the deviations from independence
<<alzheimer-maxstat, echo = TRUE>>=
statistic(it_alzmax, "standardized")
@
and leads to the impression that patients suffering 
from Alzheimer's disease
smoked less cigarettes than expected under independence and, to a much larger
degree, patients with other dementias smoked much more than expected.
However, interpreting the standardized contingency table either requires
knowledge about the distribution of the standardized statistics, 
i.e. via an approximation of the $97.5\%$ quantile of the conditional 
null distribution (two-sided test) which is available from
<<alzheimer-qperm, echo = TRUE>>=
qperm(it_alzmax, 0.975)
@
Alternatively and more conveniently, we can 
to switch to the $p$-value scale. Here, we choose step-down
adjusted resampling-based $p$-values \citep{WestfallYoung1993}. First,
we approximate the conditional distribution by $50.000$ Monte-Carlo 
replications
<<alzheimer-MC, echo = TRUE>>=
it_alzMC <- independence_test(alzheimer, 
                              distribution = approximate(B = 50000))
@
with global $p$-value
<<alzheimer-MCp, echo = TRUE>>=
pvalue(it_alzMC)
@
(the normal approximation is rather accurate)
and apply Algorithm 2.8 in \cite{WestfallYoung1993} to obtain $p$-values
adjusted for multiple comparisons
<<alzheimer-MTP, echo = TRUE>>=
pvalue(it_alzMC, method = "step-down")
@
The above results support the conclusion that the rejection of the null hypothesis of
independence is due to a large number of heavy smokers with other dementias
but seems rather unrelated to Alzheimer's disease itself.

Of course, ignoring the ordinal structure of one of the variables is only
suboptimal. Ordinal variables can be incorporated into the general framework
via linear-by-linear association tests \citep{Agresti2002}. 
When $\Y$ is measured at $J$ levels and $\X$ at $K$ levels,
$\Y$ and $\X$ are associated with score vectors $\xi \in
\R^J$ and $\gamma \in \R^K$, respectively. The linear statistic is now a linear
combination of the linear statistic $\T$ of the form
\begin{eqnarray*}
\M \T = \vec \left( \sum_{i=1}^n \gamma^\top g(\X_i)
            \left(\xi^\top h(\Y_i, (\Y_1, \dots, \Y_n)\right)^\top \right)
\in \R \text{ with } \M = \xi \otimes \gamma.
\end{eqnarray*}

For smoking, a natural choice of the scores are the midpoints of
the internals used to discretize the number of cigarettes per day 
and we can setup
a linear-by-linear association test with $c_\text{max}$ type test statistic via
<<alzheimer-ordered, echo = TRUE>>=
it_alzL <- independence_test(alzheimer, scores = list(smoking = c(0, 5, 15, 25)))
pvalue(it_alzL)
@
and the single-step adjusted $p$-values 
<<alzheimer-orderedp, echo = TRUE, eval = FALSE>>=
pvalue(it_alzL, method = "single-step")
@
<<alzheimer-orderedp, echo = FALSE>>=
p <- pvalue(it_alzL, method = "single-step")
rownames(p) <- ""
p
@
support the conclusion that smoking is associated with other dementia and, 
therefore, smoking is less frequent in patients suffering from Alzheimer's 
disease.

\paragraph{Photococarcinogenicity Experiments.}

The effect on tumor frequency and latency in photococarcinogenicity
experiments, where carcinogenic doses of ultraviolet radiation (UVR) are
administered, are measured by means of (at least) three response variables:
the survival time, the time to first tumor and the total number of tumors of
animals in different treatment groups. 
The main interest is testing the global null of no treatment 
effect with respect to survival time, time to first tumor or
number of tumors \citep[][analyze the detection time
of tumors in addition, this data is not given here]{Molefeetal2005}. 
In case the  global null hypothesis can be rejected, the deviations 
from the partial hypotheses are of special interest.

\cite{Molefeetal2005} report data of an experiment where
$\Sexpr{nrow(photocar)}$ animals were exposed to different levels 
of UVR exposure (group A: topical vehicle and 600 Robertson--Berger units 
of UVR, group B: no topical vehicle and 600 Robertson--Berger units of UVR and group C: 
no topical vehicle and 1200 Robertson--Berger units of UVR). 
The data are taken from Tables~1 to 3 in \cite{Molefeetal2005}, where a 
parametric test procedure is proposed. Figure
\ref{photocarfig} depicts the group effects for all three response
variables. 


\begin{sidewaysfigure}
\begin{center}
<<photocar-plot, echo = FALSE, fig = TRUE, width = 7, height = 3>>=
par(cex.lab = 1.3, cex.axis = 1.3)
layout(matrix(1:3, ncol = 3))
plot(survfit(Surv(time, event) ~ group, data = photocar), xmax = 50,
     xlab = "Survival Time (in weeks)", ylab = "Probability",
     lty = 1:3)
     legend("bottomleft", lty = 1:3, levels(photocar$group), bty = "n")
plot(survfit(Surv(dmin, tumor) ~ group, data = photocar), xmax = 50,
     xlab = "Time to First Tumor (in weeks)", ylab = "Probability",
     lty = 1:3)
     legend("bottomleft", lty = 1:3, levels(photocar$group), bty = "n")
boxplot(ntumor ~ group, data = photocar, 
        ylab = "Number of Tumors", xlab = "Treatment Group")
@
\caption{\Robject{photocar} data: 
         Kaplan-Meier estimates of time to death and time to first tumor as
         well as boxplots of the total number of tumors in three treatment
         groups. \label{photocarfig}}
\end{center}
\end{sidewaysfigure}

First, we construct a global test for the null hypothesis of independence
of treatment and \textit{all} three response variables. A
$c_\text{max}$-type test based on the standardized multivariate          
linear statistic and an approximation of the conditional distribution
utilizing the asymptotic distribution simply reads
<<photocar-global, echo = TRUE>>=
data("photocar", package = "coin")
it_ph <- independence_test(Surv(time, event) + Surv(dmin, tumor) + ntumor ~ group,
                           data = photocar)
it_ph 
@
Here, the influence function $h$ consists of the logrank scores of the survival
time and time to first tumor as well as the number of tumors, i.e. for the 
first animal in the first group $h(\Y_1, (\Y_1, \dots, \Y_n)) =
\Sexpr{paste("(", paste(round(it_ph@statistic@ytrans[1,], 2), collapse = ","), ")")}$
and $g(\X_1) = (1, 0, 0)$. The multivariate statistic is the sum of each of
the three elements of the influence function $h$ in each of the groups, i.e.
<<photocar-linear, echo = TRUE>>=
statistic(it_ph, type = "linear")
@
It is important to note that this global test utilizes the complete
correlation structure 
<<photocar-covar, echo = TRUE, eval = FALSE>>=
cov2cor(covariance(it_ph))
@
<<photocar-covar, echo = FALSE>>=
round(cov2cor(covariance(it_ph)), 2)
@
<<photocar-quad, echo = FALSE>>=
it_phq <- independence_test(Surv(time, event) + Surv(dmin, tumor) + ntumor ~group,
                           data = photocar, teststat = "quadtype")
@
when $p$-values are computed via quasi-randomized Monte-Carlo
procedures in the multivariate setting \citep{numerical-:1992}.
Alternatively, a test statistic based on the quadratic form $c_\text{quad}$
directly incorporates the covariance matrix and leads to a very similar 
$p$-value. 
%%%% $\Sexpr{round(pvalue(it_phq), 5)}$.

The deviations from the partial null hypotheses, i.e. independence of
each single response and treatment groups, can be inspected by the standardized
linear statistic $\T$
<<photocar-stand, echo = FALSE>>=
statistic(it_ph, type = "standardized")
@
or by means of adjusted $p$-values
<<photocar-stand, echo = TRUE, eval = FALSE>>=
pvalue(it_ph, method = "single-step")
@
<<photocar-stand, echo = FALSE>>=
round(pvalue(it_ph, method = "single-step"), 5)
@
Of course, the goodness of the asymptotic procedure can be checked against
the Monte-Carlo approximation which is computed by
<<photocar-MC, echo = TRUE>>=
it <- independence_test(Surv(time, event) + Surv(dmin, tumor) + ntumor ~ group,
                  data = photocar, distribution = approximate(50000))
pvalue(it, method = "single-step")
@
The more powerful step-down adjusted $p$-values are 
<<photocar-MC2, echo = TRUE>>=    
pvalue(it, method = "step-down")
@
Clearly, the rejection of the global null hypothesis is due to the
group differences in both survival time and time to first tumor whereas 
no treatment effect on the total number of tumors can be observed.

\paragraph{Contaminated Fish Consumption.}

In the former two applications, standard transformations for $g$ and $h$
such as dummy codings and logrank scores have been applied. In the third 
application, we will show how one can utilize the \Rpackage{coin}
functionality to implement a newly invented test procedure. 

\cite{Rosenbaum1994a} proposed to compare groups by means of a
\textit{coherence criterion} and studied a dataset of subjects 
who ate contaminated fish for more than three years in
the 'exposed' group and a control group. Three response variables are
available: the mercury level of the blood, the percentage of cells with
structural abnormalities and the proportion of cells with asymmetrical or
incomplete-symmetrical chromosome aberrations (see Figure \ref{mercurybox}). 
The observations are partially
ordered: an observation is said to be smaller than another when all three variables
are smaller. The rank score for observation $i$ is the number of
observations that are larger (following the above criterion) 
than observation $i$ minus the number of
observations that are smaller. The
distribution of the rank scores in both groups is to be compared and
the corresponding test is called `POSET-test' (partially ordered
sets).

\begin{sidewaysfigure}
\begin{center}
<<mercuryfish-plot, echo = FALSE, fig = TRUE, width = 7, height = 3>>=
par(cex.lab = 1.3, cex.axis = 1.3)
layout(matrix(1:3, ncol = 3))
boxplot(I(log(mercury)) ~ group, data = mercuryfish, 
        ylab = "Mercury Blood Level (log scale)")
boxplot(abnormal ~ group, data = mercuryfish, 
        ylab = "Abnormal Cells (in %)")
boxplot(ccells ~ group, data = mercuryfish, 
        ylab = "Chromosome Aberrations (in %)")
@
\caption{\Robject{mercuryfish} data: 
         Distribution of all three response variables in the exposed group
         and control group. \label{mercurybox}}
\end{center}
\end{sidewaysfigure}

The coherence criterion can be formulated in a simple function
<<mercurysfish-score, echo = TRUE>>=
coherence <- function(data) {
    x <- t(as.matrix(data))
    matrix(apply(x, 2, function(y)
           sum(colSums(x < y) == nrow(x)) - 
           sum(colSums(x > y) == nrow(x))), ncol = 1)
}
@
which is now defined as influence function $h$ via the \Rcmd{ytrafo} argument
<<mercuryfish-poset, echo = TRUE>>=
data("mercuryfish", package = "coin")
poset <- independence_test(mercury + abnormal + ccells ~ group, 
    data = mercuryfish, ytrafo = coherence, distribution = exact())
@
Once the transformations $g$ (a zero-one coding of the exposed and control
group) and $h$ (the coherence criterion) are defined, we enjoy the whole
functionality of the framework, including an exact two-sided $p$-value
<<mercuryfish-pvalue, echo = TRUE>>=
pvalue(poset)
@
and density (\Rcmd{dperm}), distribution (\Rcmd{pperm}) and quantile functions 
(\Rcmd{qperm}) of the conditional distribution. When only a small number of
observations is available, it might be interesting to compare the exact
conditional distribution and its approximation via the limiting distribution.
For the \Robject{mercuryfish} data, the relevant parts of 
both distribution functions are shown in Figure~\ref{distplot}. It 
turns out the the normal approximation would be sufficient for all practical
purposes in this application.

\setkeys{Gin}{width=0.8\textheight}
\begin{sidewaysfigure}
\begin{center}
<<mercuryfish-ppermplot, echo = FALSE, fig = TRUE, height = 5, width = 9>>=
par(cex.lab = 1.3, cex.axis = 1.1)
ite <- poset
ita <- independence_test(mercury + abnormal + ccells ~ group, data =     
                           mercuryfish, ytrafo = coherence)
site <- support(ite)
layout(matrix(1:2, ncol = 2))
site <- site[site <= qperm(ite, 0.1) & site > -3]
pite <- sapply(site, function(x) pperm(ite, x))
pita <- sapply(site, function(x) pperm(ita, x))

plot(site, pite, type = "S", ylab = "Distribution", xlab = "Standardized Statistic")
lines(site, pita, lty = 3)
legend("topleft", lty = c(1,3), legend = c("Conditional Distribution",
"Approximation"), bty = "n")

site <- support(ite)
site <- site[site >= qperm(ite, 0.9) & site < 3]
pite <- sapply(site, function(x) pperm(ite, x))
pita <- sapply(site, function(x) pperm(ita, x))

plot(site, pite, type = "S", ylab = "Distribution", xlab = "Standardized Statistic")
lines(site, pita, lty = 3)
@
\caption{\Robject{mercuryfish} data: 
         Conditional distribution and asymptotic normal approximation 
         for the POSET test. \label{distplot}}
\end{center}
\end{sidewaysfigure}


\section{Conclusion}

Conditioning on the observed data is a simple, yet powerful, design
principle for statistical tests. Conceptually, one only needs to choose
an appropriate test statistic and evaluate it for all admissible 
permutations of the data \citep[][gives an example with Hotelling
's $T^2$]{Ernst2004}. In practical setups, an implementation of this
procedure requires a certain amount of programming and computing time. 
Often, permutation tests are regarded as being `computationally impractical'
for larger sample sizes \citep{BalkinMallows2001}. Therefore, popular
software packages offer implementations for the most prominent conditional
tests, such as the permutation $t$ test, where either fast algorithms for
the computation of conditional $p$-values are available or the limiting
distribution is known.

The permutation test framework by \cite{StrasserWeber1999} makes at least 
two important contributions: analytic formulae for the 
conditional expectation and covariance and the limiting normal distribution
of a class of multivariate linear statistics. Thus, test statistics can be
defined for appropriately standardized linear statistics and a fast
approximation of the conditional distribution is available, even for large
sample sizes. 

The \Rpackage{coin} package is an attempt to translate the theoretical concepts of 
\cite{StrasserWeber1999} into software as closely as possible preserving the
simplicity and flexibility of conditional inference. Basically, the package
implements \textit{one} function for computing the linear statistic $\T$,
\textit{one} function for the conditional expectation $\mu$ and covariance $\Sigma$
and plug-ins for several test statistics $c$. Moreover, normal, $\chi^2$ or
Monte-Carlo approximations of the conditional distribution only need to be
implemented \textit{once}.

But who stands to benefit from such a software infrastructure? We argue 
that better data analysis is possible in cases when the appropriate 
conditional test is not available from standard software packages.
Statisticians can modify existing test procedures or even try new ideas by
computing directly on the theory. A high-level Lego-system is attractive for
software developers, because only the transformation $g$ and influence
function $h$ need to be newly implemented, but the burden of implementing a
Monte-Carlo procedure, or even thinking about asymptotics, 
is waived. Since the \Rpackage{coin} package consists
of only a few core functions that need to be tested, the setup of quality
assurance tools is rather simple in this case. Many text books
\citep[e.g.][]{HollanderWolfe1999} or software manuals \citep[first of all
the excellent StatXact handbook][]{StatXact6} include examples and results
of the associated test procedures which have been reproduced with
\Rpackage{coin}. 

Since the \Rpackage{coin} package is part of the Comprehensive \RR{} Archive
Network (CRAN, \url{http://CRAN.R-project.org}) we have been able to help
several people asking `Is the xyz-test available in \RR{}' on the
\texttt{r-help} email list with the answer `No, but its only those 
two lines of \RR{} code in \Rpackage{coin}'. With the \Rpackage{coin}
functionality being available we are no longer limited to already
implemented test procedures or forced to self-implementation. Instead, 
the appropriate conditional test procedure for the problem at hand is only a
matter of choosing appropriate transformation and influence functions. 

\bibliographystyle{asa}
\bibliography{litdbTH,addrefs}

\end{document}
