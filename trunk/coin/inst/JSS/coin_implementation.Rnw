\documentclass[article]{jss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Torsten Hothorn \\ Friedrich--Alexander--Universit\"at Erlangen--N\"urnberg \And
        Kurt Hornik \\Wirtschaftsuniversit\"at Wien \And
        Mark A. van de Wiel \\Vrije Universiteit Amsterdam \And
        Achim Zeileis\\Wirtschaftsuniversit\"at Wien}
\title{\pkg{coin}: Implementing a Class of Permutation Tests}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Torsten Hothorn, Kurt Hornik, Mark A. van de Wiel, Achim Zeileis} %% comma-separated
\Plaintitle{\pkg{coin}: Implementing a Class of Permutation Tests} %% without formatting
\Shorttitle{Implementing a Class of Permutation Tests} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  The abstract of the article.
}
\Keywords{conditional inference, \proglang{R}}
\Plainkeywords{conditional inference, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: This needs to filled out ONLY IF THE PAPER WAS ACCEPTED.
%% If it was not (yet) accepted, leave them commented.
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Torsten Hothorn \\
  Institut f\"ur Medizininformatik, Biometrie und Epidemiologie\\
  Friedrich-Alexander-Universit\"at Erlangen-N\"urnberg\\
  Waldstra{\ss}e 6, D-91054 Erlangen, Germany \\  
  Email: \email{Torsten.Hothorn@R-project.org}

  Kurt Hornik and Achim Zeileis\\
  Institut f\"ur Statistik \& Mathematik\\
  Wirtschaftsuniversit\"at Wien\\
  1090 Wien, Austria\\
  E-mail: \email{Kurt.Hornik@R-project.org}\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\

  Mark A. van de Wiel \\
  Department of Mathematics, Vrije Universiteit \\
  De Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands \\
  Email: \email{mark.vdwiel@vumc.nl}

}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\input{head}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<coin-setup, echo = FALSE, results = hide>>=
library("coin")
set.seed(290875)
### get rid of the NAMESPACE
load(file.path(.find.package("coin"), "R", "all.rda"))
@

\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

\section{Theory and Implementation}

\subsection{Data}

In the following we assume that we are provided with $n$ observations 
\begin{eqnarray*}
(\Y_i, \X_i, w_i, b_i), \quad i = 1, \dots, n.
\end{eqnarray*}
The variables $\Y$ and $\X$ from sample spaces $\mathcal{Y}$ and
$\mathcal{X}$ may
be measured at arbitrary scales and may be multivariate as well. In addition
to those measurements, case weights $w$ and a factor $b$ coding blocks may
be available. The data structure is represented by class \Rclass{IndependenceProblem}:
<<IndependenceProblem>>=
showClass("IndependenceProblem")
@

\subsection{Test Problem and Linear Statistic}

We are interested in testing the null hypothesis of independence of $\Y$ and
$\X$
\begin{eqnarray*}
H_0: D(\Y | \X) = D(\Y)
\end{eqnarray*}
against arbitrary alternatives. \cite{StrasserWeber1999} suggest to derive
scalar test statistics for testing $H_0$ from multivariate linear statistics
of the form 
\begin{eqnarray} \label{linstat}
\T = \vec\left(\sum_{i = 1}^n w_i g(\X_i) h(\Y_i, (\Y_1, \dots, \Y_n))^\top\right)
\in \R^{pq}.
\end{eqnarray}
Here, $g: \mathcal{X} \rightarrow \R^{p}$ is a transformation of
the $\X$ measurements and the \emph{influence function}
$h: \mathcal{Y} \times \mathcal{Y}^n \rightarrow
\R^q$ depends on the responses $(\Y_1, \dots, \Y_n)$ in a permutation
symmetric way. 
The transformation $g$ and influence function $h$ as well as 
$g(\X_i)$ and $h(\Y_i), i = 1, \dots, n$, are attached to the data structure
by extending class \Rclass{IndependenceProblem}:
<<IndependenceTestProblem>>=
showClass("IndependenceTestProblem")
@

\subsection{Conditional Expectation and Covariance}

The distribution of $\T$  depends on the joint
distribution of $\Y$ and $\X$, which is unknown under almost all practical
circumstances. At least under the null hypothesis one can dispose of this
dependency by fixing $\X_1, \dots, \X_n$ and conditioning on all possible
permutations $S$ of the responses $\Y_1, \dots, \Y_n$. 
This principle leads to test procedures known
as \textit{permutation tests}. 

The conditional expectation $\mu \in \R^{pq}$ and covariance 
$\Sigma \in \R^{pq \times pq}$ 
of $\T$ under $H_0$ given
all permutations $\sigma \in S$ of the responses are derived by
\cite{StrasserWeber1999}:
\begin{eqnarray}
\mu & = & \E(\T | S) = \vec \left( \left( \sum_{i = 1}^n w_i g(\X_i) \right) \E(h | S)^\top
\right), \nonumber \\
\Sigma & = & \V(\T | S) \nonumber \\
& = &
    \frac{\ws}{\ws - 1}  \V(h | S) \otimes
        \left(\sum_i w_i  g(\X_i) \otimes w_i  g(\X_i)^\top \right)
\label{expectcovar}
\\
& - & \frac{1}{\ws - 1}  \V(h | S)  \otimes \left(
        \sum_i w_i g(\X_i) \right)
\otimes \left( \sum_i w_i g(\X_i)\right)^\top
\nonumber
\end{eqnarray}
where $\ws = \sum_{i = 1}^n w_i$ denotes the sum of the case weights,
and $\otimes$ is the Kronecker product. The conditional expectation of the
influence function is
\begin{eqnarray*}
\E(h | S) = \ws^{-1} \sum_i w_i h(\Y_i, (\Y_1, \dots, \Y_n)) \in
\R^q
\end{eqnarray*}
with corresponding $q \times q$ covariance matrix
\begin{eqnarray*}
\V(h | S) = \ws^{-1} \sum_i w_i \left(h(\Y_i, (\Y_1, \dots, \Y_n))
- \E(h | S)
\right) \\
\left(h(\Y_i, (\Y_1, \dots, \Y_n)) - \E(h | S)\right)^\top.
\end{eqnarray*}

The linear statistic $\T$, its conditional expectation and covariance 
are stored in objects of class \Rclass{IndependenceTestStatistic} extending
\Rclass{IndependenceTestProblem}:
<<IndependenceTestStatistic>>=
showClass("IndependenceTestStatistic")
@
Class \Rclass{VarCovar} represents either a complete covariance matrix 
or its diagonal elements only.

\subsection{Test Statistics}

Having the conditional expectation and covariance at hand we are able to
standardize a linear statistic $\T \in \R^{pq}$ of the form
(\ref{linstat}). Univariate test statistics~$c$ mapping an observed linear
statistic $\mathbf{t} \in
\R^{pq}$ into the real line can be of arbitrary form.  An obvious choice is
the maximum of the absolute values of the standardized linear statistic
\begin{eqnarray*}
c_\text{max}(\mathbf{t}, \mu, \Sigma)  = \max \left| \frac{\mathbf{t} -
\mu}{\text{diag}(\Sigma)^{1/2}} \right|
\end{eqnarray*}
utilizing the conditional expectation $\mu$ and covariance matrix
$\Sigma$. The application of a quadratic form $c_\text{quad}(\mathbf{t}, \mu,
\Sigma)  =
(\mathbf{t} - \mu) \Sigma^+ (\mathbf{t} - \mu)^\top$ is one alternative, although
computationally more expensive because the Moore-Penrose 
inverse $\Sigma^+$ of $\Sigma$ is involved.

Three classes, all extending class \Rclass{IndependenceTestStatistic}, are defined for
the different test statistics. For univariate problems ($pq = 1$), a special class
<<ScalarIndependenceTestStatistic>>=
showClass("ScalarIndependenceTestStatistic")
@
is available. For the more general case, maximum-type statistics are represented by
objects of class \Rclass{MaxTypeIndependenceTestStatistic}:
<<MaxTypeIndependenceTestStatistic>>=
showClass("MaxTypeIndependenceTestStatistic")
@
and quadratic forms via
<<QuadTypeIndependenceTestStatistic>>=
showClass("QuadTypeIndependenceTestStatistic")
@


\subsection{Reference Distribution}

The definition of one- and two-sided $p$-values used for the computations in
the \texttt{coin} package is 
\begin{eqnarray*}
P(c(\T, \mu, \Sigma) &\le& c(\mathbf{t}, \mu, \Sigma)) \quad \text{(less)} \\
P(c(\T, \mu, \Sigma) &\ge& c(\mathbf{t}, \mu, \Sigma)) \quad \text{(greater)}\\
P(|c(\T, \mu, \Sigma)| &\le& |c(\mathbf{t}, \mu, \Sigma)|) \quad \text{(two-sided).}
\end{eqnarray*}
Note that for quadratic forms only two-sided $p$-values are available 
and that in the one-sided case maximum type test statistics are replaced by
\begin{eqnarray*}
\min \left( \frac{\mathbf{t} - \mu}{\text{diag}(\Sigma)^{1/2}} \right) 
    \quad \text{(less) and } 
\max \left( \frac{\mathbf{t} - \mu}{\text{diag}(\Sigma)^{1/2}} \right) 
    \quad \text{(greater).}
\end{eqnarray*}

The conditional distribution and thus the $p$-value
of the statistics $c(\mathbf{t}, \mu, \Sigma)$ can be
computed in several different ways. For some special forms of the
linear statistic, the exact distribution of the test statistic is trackable.
For two-sample problems, the shift-algorithm by \cite{axact-dist:1986} 
and \cite{exakte-ver:1987} and the split-up algorithm by 
\cite{vdWiel2001} are implemented as part of the package.
Conditional Monte-Carlo procedures can be used to approximate the exact
distribution. \cite{StrasserWeber1999} proved (Theorem 2.3) that the   
conditional distribution of linear statistics $\T$ with conditional    
expectation $\mu$ and covariance $\Sigma$ tends to a multivariate normal
distribution with parameters $\mu$ and $\Sigma$ as $n, \ws \rightarrow
\infty$. Thus, the asymptotic conditional distribution of test statistics of
the
form $c_\text{max}$ is normal and
can be computed directly in the univariate case ($pq = 1$)
or approximated by means of quasi-randomized Monte-Carlo  
procedures in the multivariate setting \citep{numerical-:1992}. For
quadratic forms
$c_\text{quad}$ which follow a $\chi^2$ distribution with degrees of freedom 
given by the rank of $\Sigma$ \citep[see][Chapter 29]{johnsonkotz1970}, exact
probabilities can be computed efficiently.


%%% ToDo: Blocks!

\end{document}
