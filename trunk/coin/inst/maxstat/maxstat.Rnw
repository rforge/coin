
<<style, echo = FALSE, results = tex>>=
style <- "Biometrics"
#style <- "Z"
if (style == "Z")
    cat("\\input{headerZ}\n")
if (style == "Biometrics")
    cat("\\input{headerBiometrics}\n")
@

%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, echo=FALSE, results=hide}

<<packages>>=
rseed <- 20061103
library("xtable")
library("survival")
@


\setcounter{page}{1}

\section{Introduction} \label{sec:introduction}

Dichotomization of variables measured at higher scale levels 
prior to model building is bad practice 
\citep[][among many others]{Royston2006}.
It will result in loss of power and sophisticated regression
models that adapt themselves to the complexity of the regression 
problem at hand are widely available. However, simple regression
models capturing step-shaped relationships
between two variables (such as a single jump in the mean function) 
are valuable for the implementation of scientific results into the 
real world: a one-parameter `good--poor' or `high--low' 
decision rule is attractive to practitioners because of its simplicity.

Such rules of thumb are frequently used to investigate new predictor
variables for patient survival in oncology. \cite{Galon2006} estimate
cutpoints for various characteristics of immune cells within colorectal
tumor samples, such as type, density or location, with respect to their
ability to differentiate between patients with good and poor prognosis.
\cite{Buccisano2006} obtain a threshold for
residual leukemic cells in acute myeloid leukemia patients from 
maximally selected log-rank statistics.
%%The ability for
%%expression levels of HER2 and co-amplified genes to predict breast cancer
%%survival is investigated by \cite{Vinatzer2005} utilizing maximally selected
%%log-rank statistics. 
Beyond applications in oncology, 
the identification of ecological thresholds is of
increasing interest \citep[see][]{Huggett2005},
e.g., the estimation of cutpoints for habitat factors discriminating between 
ecosystems with low and high abundance of certain indicator species 
\citep{Muller2004}.

Two questions arise from a statistical point of view. In a first step, we
have to make sure that there is some 
relevant association between response and covariate 
and in a second step we want to estimate the `best' cutpoint 
in order to approximate this relationship by a simple model. 
It is convenient to deal with both problems separately. The first problem needs to be addressed by
a formal hypothesis test for the null hypothesis of independence between covariate 
(to be dichotomized)
and response variable. A test with power against shift alternatives, i.e.,
departures from the null hypothesis where the distribution of the response variable 
varies between two groups of observations, is of special interest. 
Once we are able to reject the null hypothesis, we are interested in the 
alternative which led to the rejection, i.e., want to estimate a cutpoint
or partition.

The first procedure of this kind, utilizing the maximum over multiple $\chi^2$ statistics
for $2 \times 2$ tables, was described by \cite{MillerSiegmund1982}. 
\cite{LausenSchumacher1992} derived an approximation for the asymptotical
distribution of maximally selected rank statistics, extending the area of 
application
to continuous and censored response variables. \cite{Betensky1999} propose
a maximally selected $\chi^2$ test for nominal response variables measured at
$k > 2$ levels and ordered categorical data.
%% (maximally selected Cochran-Armitage test).
%%Finally, \cite{Rabinowitz2000} suggested a maximally selected McNemar's test.
%%\cite{Lausenetal2004} extended maximally selected rank statistics to more 
%%than one covariate.

Based on the ideas underlying these established techniques, we suggest a new
generalized class of maximally selected statistics that contains the statistics
sketched above as special cases but also allows for direct construction of new test
procedures for less standard test problems. For evaluating the distribution of
the test statistics, a conditional inference approach is adopted by embedding
the tests into the theory of permutation tests of \cite{StrasserWeber1999}.
This permits efficient computation of the complete correlation structure
of the statistics to be maximized. For statistics derived from cutpoints,
the correlations have a special product form which we exploit for evaluation
of the conditional asymptotic distribution: A linear-time algorithm is described
which enables the fast assessment of a large number of cutpoints and
improves upon approximations currently in use. 


For illustrating the flexibility of the new framework for generalized maximally selected  
statistics we exemplify how the methodology can be extended to new areas of
application by constructing 
a maximally selected log-rank statistic for a censored response partitioned with
respect to two ordered categorical covariates and potential interactions.
This new test is employed to search for a high-risk group determined by the T and N-category
of rectal cancer patients. Further novel procedures include maximally selected statistics for 
multivariate responses or maximally selected permutation tests.
%%Although the above sketched procedures are quite popular, most of them rely
%%on rather rough approximations of the reference distribution. The exact conditional 
%%distribution for small samples has been derived for binary response variables 
%%by \cite{Boulesteix2006a,Boulesteix2006b} and an upper bound for the exact $p$-value
%%in the more general case of binary, numeric and censored responses is given
%%by \cite{HothornLausen2003}. The unconditional asymptotic distribution is the distribution
%%of the maximum of the absolute values of standard normal variables with known
%%correlation structure \cite{HothornLausen2003}. For a potentially large number
%%of cutpoints, this distribution is hard to evaluate and approximations, based
%%on the supremum of a Brownian bridge or Ornstein-Uhlenbeck processes and
%%an improved Bonferroni inequality, have to be used. As \cite{HothornLausen2003}, Section 6,
%%point out, those approximations are rather inefficient even for a large number of
%%possible cutpoints and number of observations.

\section{Binary partitions and two-sample statistics} \label{sec:stat}

%% more than binary partitions?
We are provided with independent and identically distributed
observations $(\Y_i, \X_i)$ for $i = 1, \dots, n$ and 
are interested in testing the null hypothesis of independence of the response 
variable $\Y \in \mathcal{Y}$ and covariate(s) $\X \in \mathcal{X}$
\begin{eqnarray*}
H_0: D(\Y | \X) = D(\Y)
\end{eqnarray*}
against shift alternatives. That is, departures from the null hypothesis
where the distribution $D(\cdot)$ of the response variable varies between two groups
of observations (with respect to $\X$) are of special interest.

Such binary partitions are defined in advance by $p$ candidate sets $A_1, \dots, A_p$.
Each set $A_j$ partitions the observations into two 
groups based on the covariate(s) only. For an ordered univariate covariate 
$\X$, these sets are
typically constructed via cutpoints, i.e., $A_j = \{\X | \X \le \xi_j\}$. 
When $\X$ is a factor at $k$ levels, there are $p = 2^{k-1}$ possible
partitions of the observations into two samples. For multivariate covariates, the
$A_j$ can code splits in interactions of the components of $\X$.
A simple zero--one dummy coding for the $j$th partition is 
$g_j(\X) = \indic(\X \in A_j)$
where $\indic$ denotes the indicator function. 
Only partitions satisfying a sample size 
constraint $\sum_i g_j(\X_i) \in (n\varepsilon, n - n\varepsilon)$ 
for some fixed $\varepsilon \in (0, 0.5)$ are taken into account (typically 
$\varepsilon = 0.1$). 

The two-sample problem associated with the $j$th binary partition
can be tested using a linear statistic
\begin{eqnarray*}
\T_j \quad = \quad \vec\left(\sum_{i = 1}^n g_j(\X_i) h(\Y_i)^\top\right) 
             \in \R^{q \times 1}
\end{eqnarray*}
where $h: \mathcal{Y} \rightarrow
\R^{q \times 1}$ is an \emph{influence function} applied to the responses.
The function $h(\Y_i) = h(\Y_i, (\Y_1, \dots, \Y_n))$ may depend on the full 
vector of responses $(\Y_1, \dots, \Y_n)$, however only
in a permutation symmetric way, i.e., the value of the
function must not depend on the order in which $\Y_1, \dots, \Y_n$ appear.
For example, with $h$ being a rank transformation for a 
continuous response $\Y$, the linear
statistic $\T_j$ is the sum of the ranks for observations from $A_j$, 
i.e., equals the Wilcoxon-Mann-Whitney statistic. When $\Y$ is a factor 
measured at $k$ levels, $h \in \R^{(k - 1) \times 1}$ 
is the corresponding dummy coding and $\T_j$ corresponds
to the $2 \times k$ contingency table of the transformation $g_j(\X)$ and response 
$\Y$. 
%%(Because the margins are fixed, we only need to compute the first $k - 1$ columns
%%of the first row of this contingency table.)

A joint linear statistic for all binary partitions is
\begin{eqnarray*}
\T \quad = \quad (\T_1, \dots, \T_p) = \vec\left(\sum_{i = 1}^n g(\X_i) h(\Y_i)^\top\right)
\in \R^{pq \times 1}
\end{eqnarray*}
including all $p$ two-sample partitions, as defined by $g(\X) = (g_1(\X), \dots, g_p(\X))$,
simultaneously for testing $H_0$.

\section{Standardization and estimation} \label{sec:inf}

To assess the partitions/cutpoints on a common scale, the 
corresponding statistics $\T_j$ 
are typically standardized using some location and scale measure. 
Consequently, inference can be based on the maximally selected 
absolute standardized statistics and the best separating partition 
is the one for which the maximum is attained.

For obtaining valid estimates of the mean and covariance of $\T$, 
either a parametric model needs to be specified or non-parametric techniques
can be employed, such as permutation or re-sampling approaches.
Here, we adopt the latter and utilize the permutation test framework
established by \cite{StrasserWeber1999}. Thus, $\T$ is standardized
via its conditional expectation $\mu = \E(\T | S) \in \R^{pq \times 1}$
and covariance $\Sigma = \V(\T | S) \in \R^{pq \times pq}$, derived under
$H_0$ by conditioning on all possible permutations $S$ of the responses
$\Y_1, \dots, \Y_n$. Closed-form expressions are
as given by \cite{StrasserWeber1999}:
\begin{eqnarray*}
\mu = \E(\T | S) & = & \vec \left( \left( \sum_{i = 1}^n g(\X_i) \right)
\E(h | S)^\top \right) \\
\Sigma = \V(\T | S) & = &
    \frac{n}{n - 1}  \V(h | S) \otimes
        \left(\sum_i g(\X_i) \otimes  g(\X_i)^\top \right)
\\
& - & \frac{1}{n - 1}  \V(h | S)  \otimes \left(
        \sum_i g(\X_i) \right) \otimes \left( \sum_i g(\X_i)\right)^\top
\nonumber
\end{eqnarray*}
where $\otimes$ denotes the Kronecker product, and the conditional
expectation of the influence function is $\E(h | S) = n^{-1} \sum_i
h(\Y_i)$ with corresponding $q \times q$ covariance matrix
$\V(h | S) = n^{-1} \sum_i \left(h(\Y_i) - \E(h | S) \right) \left(h(\Y_i) - \E(h | S)\right)^\top$.

When the observations are organized in independent blocks (such as centers
in a multicenter trial), only permutations within blocks are admissible 
and thus expectations and covariance matrices have
to be computed separately within each block. The expectation $\mu$ and covariance
matrix $\Sigma$ of $\T$ are then obtained as the sum over all expectations and
covariance matrices.
Therefore, it is easily possible to take a block randomization
scheme in a randomized clinical trial or dependent sample designs 
into account.

The key step for constructing a maximally selected 
statistic
%% based on the multivariate
%% linear statistic $\T$ is its 
is the standardization of $\T$ by its 
conditional expectation $\mu$ and covariance matrix $\Sigma$: the 
test statistic is the absolute maximum of the standardized linear statistic
\begin{eqnarray*}
\Tmax  & = & \max \frac{|\T - \mu|}{\sqrt{\text{diag}(\Sigma)}}.
\end{eqnarray*}
When the test statistic is large enough to indicate a deviation 
from the null hypothesis we are interested in determining the
partition with largest standardized statistic: the best
separating partition $A_{j^\star}$ is the one for which the maximum is
attained, i.e., for which the absolute value of the standardized
statistic $\T_{j^\star}$ equals $\Tmax$.
%%\begin{eqnarray*}
%%j^\star = \argmax_j \left|\T_j - \mu_j\right| \times \Sigma_{j,j}^{-1/2}.
%%\end{eqnarray*}
%Z% Either use the same layout as above or simply refer to the argmax
%Z% in the formula above.

\section{Inference}

%% conditional distribution of Tmax: exact, approx, asympt (alles muehsam)
%Z% ...not anymore ;-)
For testing $H_0$, the conditional distribution
of $\Tmax$ given all permutations of the responses is used as reference distribution.
Ideally, we want to compute the exact conditional distribution but this is only
possible in special small sample situations \citep{Boulesteix2006b,Boulesteix2006a,Boulesteix:2007}.
Conditional Monte-Carlo methods can be used to approximate the exact conditional
distribution rather easily: evaluate the test statistic $\Tmax$ for a large number of
randomly shuffled responses $\Y$ and compute the $p$-value as proportion of
permuted statistics that exceed the observed statistic.

Moreover, the exact conditional distribution can be approximated by its
limiting distribution. For $n \rightarrow \infty$ the
distribution of the multivariate linear statistic $\T$ tends to a multivariate 
normal distribution with mean $\mu$ and covariance matrix $\Sigma$ 
\citep[Theorem 3]{StrasserWeber1999}.
Thus, in order to approximate $\Prob(\Tmax > c)$ we have to evaluate
the probability $\Prob(\max(|Z_1|, ..., |Z_{pq}|) > c)$ 
for standard normal random variables $Z_1, \dots, Z_{pq}$ with correlation matrix 
$\Rb$ corresponding to the covariance matrix $\Sigma$ and some $c > 0$. 
%%In fact,
%%this conditional asymptotical distribution coincides with the unconditional asymptotical
%%distribution obtained for maximally selected rank statistics \citep[e.g.,][]{HothornLausen2003}.
The computation of this probability is possible using Quasi-Monte-Carlo methods 
\citep{Genz1992} for moderate dimensions ($pq < 100$, say) but remains infeasible
for higher dimensions.
%%So far, even the asymptotic distribution of $\Tmax$ had to be approximated by
%%approximations of the distribution of the supremum of a Brownian bridge \citep{MillerSiegmund1982} 
%%or Ornstein-Uhlenbeck process \citep{LausenSchumacher1992} over some interval for higher
%%dimensions ($pq > 50$, say). 
%%It should be noted that both approximations ignore the 
%%special correlation structure. \cite{LausenSchumacher1996} proposed to
%%use an improved Bonferroni inequality presented by \cite{Worsley1982} which
%%provides us with an upper bound for $\Prob(\Tmax > c)$ as $n \rightarrow \infty$.
%% (see Appendix).
However, for the most important case of statistics maximally selected
over cutpoints induced by an ordered covariate $\X$ and an ordered, censored 
or binary response $\Y$, the distribution 
can be evaluated numerically by an algorithm with
computing time being linear in the number of cutpoints $p$ as will be shown 
in the following.

\section{A new and fast approximation}

Let $A_j = (-\infty, \xi_j]$ with $\xi_j < \xi_k$ for $1 \le j < k \le p$ 
denote the partitioning sets and let $q = 1$ (i.e., ordered, censored or binary response variable). 
Then, the correlation between $\T_j$ and $\T_k$ is given by
\begin{eqnarray*}
\rho_{j,k} = \frac{\Sigma_{j,k}}{\sqrt{\Sigma_{j,j} \Sigma_{k,k}}} = 
\sqrt{\frac{\left(n - \sum_i g_k(\X_i)\right) \sum_i g_j(\X_i)}
                        {\left(n - \sum_i g_j(\X_i)\right) \sum_i g_k(\X_i)}}.
\end{eqnarray*}
It follows that the correlation matrix $\Rb = (\rho_{j,k})_{j,k = 1, \dots, p}$
is completely determined by the 
subdiagonal elements $\rho_{j,j-1}, j = 2, \dots, p$ and it holds that 
\begin{eqnarray*}
\rho_{1,k} = \prod_{j = 2}^k \rho_{j,j-1}.
\end{eqnarray*}
With $\mathbf{v} = (\rho_{1,1}, \dots, \rho_{1,p})$ the lower triangular part of $\Rb$ 
can be written as $\mathbf{v} (1/\mathbf{v})^\top$ and it follows 
from \citet[Section 2.1]{Meurant1992} that the
inverse $\Rb^{-1}$ of the correlation matrix is a tridiagonal symmetric band matrix:
\begin{eqnarray*}
\Rb^{-1} = \left( \begin{array}{cccccc}
r_{1,1} & r_{1,2} & 0       & 0 & \dots & 0 \\
r_{1,2} & r_{2,2} & r_{2,3} & 0 & \dots & 0 \\
      0 & r_{2,3} & r_{3,3} & r_{3,4} & \dots & 0  \\
      0 & 0 & r_{3,4} & r_{4,4} & \ddots & 0  \\
\vdots & \vdots   &  \vdots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & 0 & r_{p-1,p-1}& r_{p-1,p} \\
0 & 0 & 0 & 0 & r_{p-1,p} & r_{p,p} 
\end{array}
\right).
\end{eqnarray*}
The probability that any of $|Z_1|, \dots, |Z_p|$ exceeds
$c > 0$ is 
\begin{eqnarray*}
\Prob(\Tmax > c) & = & 1 - 
\frac{1}{\sqrt{|\Rb| (2\pi)^p}} \int\limits_{-c}^{c} 
\exp\left(-\frac{1}{2}\z^\top \Rb^{-1}\z \right) d\z.
\end{eqnarray*}
Due to the band structure of $\Rb^{-1}$ the quadratic form $\z^\top \Rb^{-1}\z$ 
simplifies to
\begin{eqnarray*}
\z^\top\Rb^{-1}\z = r_{1,1}z_1^2 + 2r_{2,1}z_1z_2 + r_{2,2} z_2^2 + \dots +
2r_{p,p-1} z_pz_{p-1} + r_{p,p}z_p^2
\end{eqnarray*}
which is employed for evaluating the
multivariate normal distribution numerically \citep{GenzKahaner1986}.
With $\phi(z) = \exp\left(-z/2\right)$ we have
\begin{eqnarray*}
& & \int\limits_{-c}^{c} \phi\left(\z^\top \Rb^{-1}\z \right) d\z = \\
& & \int\limits_{-c}^c \phi(r_{1,1}z_1^2)
\int\limits_{-c}^c \phi(2r_{2,1}z_1z_2 + r_{2,2} z_2^2)
\int\limits_{-c}^c \dots
\int\limits_{-c}^c \phi(2r_{p,p-1} z_pz_{p-1} + r_{p,p}z_p^2) d \z
\end{eqnarray*}
and with recursively defined functions $f_j$ ($j = 2, \dots, p + 1$)
\begin{eqnarray*}
f_j(z) = \int\limits_{-c}^c \phi\left(2 r_{j, j-1} z \tilde{z} + r_{j,j} \tilde{z}^2\right)
             f_{j+1}(\tilde{z}) d\tilde{z} 
            \quad \forall j = 2, \dots, p; \quad 
f_{p+1}(z) \equiv 1
\end{eqnarray*}
the above integral can be re-formulated recursively:
\begin{eqnarray*}
\Prob(\Tmax > c) & = & 1 - \frac{1}{\sqrt{|\Rb| (2\pi)^p}} \int\limits_{-c}^{c} 
\phi\left(\z^\top \Rb^{-1}\z \right) d\z \\
& = & 1 - \frac{1}{\sqrt{|\Rb| (2\pi)^p}}
\int\limits_{-c}^c \phi(r_{1,1}z^2) f_2(z) dz.
\end{eqnarray*}
This integral can be evaluated numerically in $O(p)$ starting with $f_p$
utilizing the techniques described by \cite{Miwa2000}: For a two-dimensional grid
of $z \in [-c, c]$ and $\tilde{z} \in [-c,c]$ values, the function $f_j$ is
evaluated and aggregated over $\tilde{z}$ only, yielding values
of $f_j(z)$ for a grid of $z$ values. 
These values are then re-used when computing $f_{j-1}$. 

Comparing this new approximation of the asymptotic distribution 
with previously suggested approximations (see Figure~\ref{approx}),
it should be pointed out that these approximations differ
with respect to the asymptotics for $p$, the number of cutpoints. In a conditional
framework, it is most natural to treat $p$ as fixed (given the observed data). Taking
an unconditional view, it depends on the partition-generating mechanism whether
$p$ is fixed as $n \rightarrow \infty$ or increases. The former holds 
for splits at sample quantiles for numeric covariates or for splits in
categorical variables where $p = k - 1$ splits are possible for ordinal factors
or $p = 2^{k-1}$ for unordered factors. However, if all possible splits in a
continuous covariate $\X$ are considered, then $p \rightarrow \infty$ as $n \rightarrow \infty$
and the sequence of test statistics $Z_1, \dots, Z_p$ is known to converge to
a stochastic Gaussian process with continuous paths:
  \[ Z^0(t) \quad = \quad \frac{B^0(t)}{\sqrt{t (1 - t)}}, \quad t \in [0, 1] \]
where $B^0(t)$ is a Brownian bridge that is scaled to zero mean and unit
variance \citep{MillerSiegmund1982}. 
The correlation of $Z^0(s)$ and $Z^0(t)$ for $s \le t$ is 
$\sqrt{s (1 - t)}/ \sqrt{t (1 - s)}$
and is exactly the same as above. 
More formally, with $t_j = \lim_{n \rightarrow \infty} n^{-1} \sum_i g_j(\X_i)$,
$Z_j$ and $Z^0(t_j)$ are asymptotically identical in distribution.
Therefore, the difference between the two approaches is that for increasing $p$
the asymptotical distribution is given by $\sup_{t \in [\varepsilon, 1 - \varepsilon]} Z^0(t)$
whereas for fixed $p$ it is $\max_{t \in \{t_1, \dots, t_p\}} Z^0(t)$.
Thus, the supremum over the full interval will always be larger than the 
maximum over a subset of times/partitions because of the additional variation
in the intervals $(t_j, t_{j + 1})$. The difference between the
two approaches decreases with $t_{j+1} - t_j$.
Figure~\ref{approx} shows that the exact conditional distribution (approximated by
$30,000$ permutations) is most closely captured by the 
conditional asymptotic distribution
suggested above. Less accurate is the improved Bonferroni correction
\citep{Worsley1982} which also uses a fixed $p$. The two approximations 
%%of the $\sup Z^0$ distribution 
for increasing $p$ \citep{Jennen:1985,Hansen:1997}
are (not surprisingly) clearly below.
\begin{figure}
\begin{center}
\caption{Approximations of the distribution of a maximally selected Wilcoxon-statistic
         for $n = 100$ observations and $p = 20$ cutpoints.
	 The exact distribution as approximated by $30,000$ random permutations of the data is
         shown as a solid line, most closely approximated by the
	 conditional asymptotic distribution suggested here. \label{approx}}
<<approx, echo = FALSE, fig = TRUE, width = 7, height = 6>>=
load("example/approx.rda")
psupT <- function(q, from = 0.1, to = 1 - from, lower.tail = TRUE, method = c("Jennen", "Hansen"))
{
  method <- match.arg(method)
  lambda <- (to * (1 - from)) / ((1 - to) * from)

  if(method == "Jennen") {
    pval <- function(x) 4 * dnorm(x)/x + dnorm(x) * (x - 1/x) * log(lambda)
  } else {
    pval <- function(x) strucchange::pvalue.Fstats(x^2, type = "supF", k = 1, lambda = lambda)
  }

  rval <- sapply(q, pval)
  if(lower.tail) rval <- 1 - rval
  return(rval)
}
ph <- psupT(g, method = "Hansen")

plot(g, ex, type = "l", lty = 1, xlab = expression(c), ylab = expression(P(T[max] <= c)))
lines(g, pex, lty = 2)
lines(g, pap, lty = 3) 
lines(g, ph, lty = 4)
lines(g, pl, lty = 5)
legend("bottomright", lty = 1:5, 
       legend = c("conditional Monte Carlo", 
                  "conditional asymptotic", 
                  "fixed p: improved Bonferroni, Worsley (1982)", 
                  "increasing p: Hansen (1997)",
                  "increasing p: Jennen (1985)"),  
       bty = "n")
@
\end{center}
\end{figure}

These considerations about the asymptotic behavior of $p$ also raise the
question about the quality of the asymptotic
approximation for finite samples when $p$ is large compared to $n$.
The joint approximation is appropriate if the normal approximation
for each two-sample statistic is. Consider a split 
in a categorical variable with large number of categories, e.g.,
$10$ categories with $10$ observations each leads to $n = 100$ but
$p = 2^{10 - 1} = 512$. But since each two-sample statistic is based on at least
$10$ and $90$ observations, respectively, the normal approximation
should work well enough.

\section{Applications and special cases}

Maximally selected statistics as described in Section~\ref{sec:stat} can be
applied to covariates $\X$ and responses $\Y$ measured at arbitrary scales; appropriate 
influence functions $h$ for nominal, ordered, numeric, censored and multivariate response
variables are given in the sequel \citep[see][for further details]{Hothornetal2006}, 
followed by a description of how to
partition the covariate space for nominal, ordered and multivariate covariates and 
the derivation of a novel maximally selected statistic.

For categorical responses, $h$ is typically a simple dummy
coding for nominal $\Y$ and a vector of numeric scores
(corresponding to the $k$ levels) for ordinal $\Y$.
Many possible influence functions are available for discrete or continuous covariates, e.g.,
identity, square root, log, or rank transformations; and for censored responses
log-rank or Savage scores can be applied.
%%For a multivariate response, possibly consisting of variables with different scale levels,
%%the influence function $h$ is a combination of 
%%influence functions appropriate for any of the univariate response variables
%%as suggested above. 

The most important situation of a univariate and (at least) ordinally measured covariate 
$\X$ leads to partitions, and thus functions $g_j$, 
induced by cutpoints defined by the realizations $\X_1, \dots, \X_n$. More specifically,
$A_j = (-\infty, \xi_j]$, where $\xi_j$ is the $j$th element of the increasingly sorted unique 
realizations of $\X$. Thus, having identified the best separating partition $A_{j^\star}$,
the estimated cutpoint is $\xi_{j^\star}$. For nominal covariates, all $2^{k-1}$ binary
partitions of the $k$ levels are considered.
For multiple covariates, we simply look at all binary partitions induced by interactions
of all covariates simultaneously. 
%%In all three cases, only partitions that meet the sample
%%size constraints are taken into account.

This flexible framework can now be utilized to implement a wide
variety of already published as well as novel maximally selected statistics.
One should bear in mind that we always utilize the conditional null distribution
which might differ from the unconditional distribution as pointed out above.

\paragraph{Maximally selected $\chi^2$ statistics.} 
The response variable is a factor at two levels $a$ and $b$, say, and $h(\Y_i) = I(\Y_i = a)$
is a dummy coding.
The ordered univariate covariate $\X$ offers $p \le n - 1$ cutpoints $\xi_1, \dots, \xi_p$ 
leading to $g_j(\X_i) = I(\X_i \le \xi_j)$ for $j = 1, \dots, p$. Thus,
\[
\T_j = \sum_{i = 1}^n g_j(\X_i) h(\Y_i)^\top = \sum_{i = 1}^n I(\X_i \le \xi_j) I(\Y_i = a) \in \R
\]
is the number of observations $i$ with $\X_i \le \xi_j$ and
$\Y_i = a$. $\T_j$ is one element of the $2 \times 2$ contingency
table for $I(\X \le \xi_j)$ and $I(\Y = a)$ and determines the 
complete table because the margins are fixed. 
The statistic $(\T - \mu)^2 / \text{diag}(\Sigma) \in \R^{p \times 1}$
is equivalent to the $p$-vector of $\chi^2$ statistics for all $p$ tables
and our maximally selected 
statistic $\Tmax$ is a monotone transformation of the
maximally selected $\chi^2$ statistic proposed by 
\cite{MillerSiegmund1982}.
For nominal responses $\Y$ with $k > 2$ levels, the statistic 
$\T_j$ corresponds to the first $k - 1$ columns of the first
row of the $2 \times k$ contingency table of $\X_i \le \xi_j$ and $\Y$.
The $\Tmax$ statistic is the maximum over the maximum of $p$ standardized contingency tables, 
an alternative to maximally selected $\chi^2$ statistics for larger tables 
\citep{Betensky1999}. 
%%In the extreme case of each observation being a block in repeated measurements, 
%%our test statistic $\Tmax$ for binary responses 
%%corresponds to maximally selected McNemar's statistics \citep{Rabinowitz2000}.

\paragraph{Maximally selected Cochran-Armitage statistics.}
The ordered response $\Y$ is measured at $k$ ordered levels. The influence
function $h$ assigns a score $\gamma_j$ to each level $j = 1, \dots, k$.
For the special case $\gamma_j = j$ this corresponds to the Cochran-Armitage test
and consequently the statistic $\Tmax$  is equivalent to a maximally 
selected Cochran-Armitage statistic \citep{Betensky1999}. For arbitrary scores,
the resulting test is a maximally selected test based on linear-by-linear
association statistics. %% \citep{agresti2002}.

\paragraph{Maximally selected rank statistics.} 
Let $h$ denote the rank transformation of a univariate response $\Y$.
Then, the statistic
\[
\T_j = \sum_{i = 1}^n I(\X_i \le \xi_j) h(\Y_i)^\top = \sum_{i: \X_i \le \xi_j} \text{rank}(\Y_i)
\]
is the sum of the ranks for all observations with $\X_i \le \xi_j$, i.e., the
Wilcoxon rank sum statistic. More generally, $h$ can be any rank transformation
(normal scores, median scores, log-rank scores etc.) and the 
linear statistic $\T_j$ is equivalent to a linear rank statistic 
\citep{HajekSidak1999}. Consequently, $\Tmax$ is equivalent to 
a maximally selected rank statistic in the sense of 
\cite{LausenSchumacher1992, LausenSchumacher1996} and \cite{HothornLausen2003}.

\paragraph{Maximally selected statistics for multiple covariates.}
When multiple covariates are under test simultaneously, we
consider all unique  partitions induced by all possible cutpoints in
each covariate. For an ordered response, this special case of 
maximally selected rank statistics for multiple covariates has first
been studied by \cite{Lausenetal2004}.

\paragraph{Three novel maximally selected statistics.}
Due to the flexibility of the generalized framework we can
easily construct tests adapted to specific problems by choosing a suitable
set of candidate partitions $g$ and transformation of the response $h$.
Here, we exemplify three 
applications: maximally selected permutation tests, maximally selected 
statistics for multivariate responses and maximally selected statistics
for interactions. Instead of using a rank-based transformation, it is
often more natural to use the original observations, i.e., employ
the identity transformation $h(\Y) = \Y$. Thus, each linear statistic $\T_j$ corresponds to
a two-sample permutation test for location alternatives. 
For a multivariate response, such as 
abundances of multiple species under investigation
\citep{Death2002}, the influence function $h$ is a combination of 
influence functions appropriate for any of the univariate response variables
as suggested above. Finally, for multiple covariates, we can not only
combine the partitions $g$ for each individual covariate---corresponding
to splits in one covariate at a time---but also employ splits in interactions
of the covariates. Contrary to previously suggested maximally selected
procedures \citep{Lausenetal2004} or recursive splitting algorithms
such as CART \citep{CART1984}, we can simultaneously search for splits
in more than one variable and thus capture interactions like the
well-known XOR problem. Below, such a strategy is employed for splitting
in two ordinal covariates \citep[an approach to splitting in SNP-SNP interactions 
is given in][]{Boulesteix+Strobl:2007}. 
To reflect the ordering, it is natural to include only
those interactions that correspond to a single cutpoint in each covariate
given the level of the other (and vice versa). Thus, interactions that would
correspond to multiple cutpoints in one covariate given the level of the other
are excluded from the set of all potential interactions.


<<cao-data>>=
<<tntab, echo = FALSE, results = tex>>=
load("example/maxstat.rda")
tc <- preOP$tclass
nc <- preOP$nclass
levels(tc) <- gsub("p", "yp", levels(tc))
levels(nc) <- gsub("p", "", levels(nc))
tab <- xtabs(~ tc + nc, data = preOP)
names(dimnames(tab)) <- c("T category", "N category")
@

%% nur geordnete zulassen?
\renewcommand{\arraystretch}{1}
\begin{table}
\begin{center}
\caption{Pathological T and N category of $\Sexpr{length(tc)}$ rectal cancer patients 
         treated with a preoperative chemoradiotherapy. \label{tab:tn}}
<<tntab, echo = FALSE, results = tex>>=
tab <- cbind(tab, rowSums(tab))
tab <- rbind(tab, colSums(tab))
if (style == "Z") {
    cat("\\begin{tabular}{|r|rrr|r|} \\hline \n")
    cat(" & \\multicolumn{3}{|c|}{N category} & \\\\ \n")
} else {
    cat("\\begin{tabular}{rrrrr} \n")
    cat(" & \\multicolumn{3}{c}{N category} & \\\\ \n")
}
cat("T category & ", paste(levels(nc), collapse = " & "), " & Total \\\\ \\hline ")
for (i in levels(tc))
cat(i, " & ", paste(tab[i,], collapse = " & "), "\\\\", ifelse(i=="ypT4", "\\hline", ""), "\n")
cat("Total & ", paste(tab[nrow(tab),], collapse = " & "), "\\\\ \\hline", "\n")
cat("\\end{tabular} \n")
@
\end{center}
\end{table}

\setkeys{Gin}{width = 0.7\textwidth}
<<gin, echo = FALSE, results = tex>>=
if (style == "Biometrics")
    cat("\\setkeys{Gin}{width = 0.95\\textwidth}\n")
@
\begin{figure}
\begin{center}
\caption{Survival times of rectal cancer patients in two risk groups 
         identified by a novel maximally selected log-rank
         statistic based on interactions of T and N category. 
         \label{fig:maxstat}}
<<maxstat, echo = FALSE, fig = TRUE, width = 7, height = 6>>=
###layout(matrix(1:2, ncol = 2))
###plot(abs(stat), xlab = "Index", ylab = "T")
risk <- as.factor(cutpoint)
plot(survfit(Surv(time, event) ~ risk, data = preOP), xlab = "Time (in months)",
     ylab = "Overall Survival Probability")
text(85, 0.92, paste("N0 or N1 (excluding N1 and ypT4); n =",table(risk)[2]))
text(85, 0.30, paste("N2 and N3 (plus N1 and ypT4); n =", table(risk)[1]))
@
\end{center}
\end{figure}

\section{Illustration}

%%Going beyond these established techniques is also easily possibly in less
%%standard situations: New tests can be constructed by choosing an influence function 
%%$h(\cdot)$ determined by the scale level of the response, and selecting a set of potential
%%partitions $g(\cdot)$ determined by the available covariates.

The TNM staging system \citep{Sobin+Wittekind:2002} for classifying the extent of cancer spread
was developed on the basis of pathologic findings of patients who did 
not undergo radiochemotherapy before surgical resection. The quality of the TNM staging 
as a prognostic factor for survival times of 
rectal patients patients treated before surgery is recently under investigation
\citep[e.g.][]{Roedel:2007}. 
Here, we attempt to identify high- and low-risk groups of
rectal cancer patients---treated with a neo-adjuvant chemoradiotherapy regime---by
differentiating them with respect to their combination of pathological T and N category
and compare the resulting partition with the TNM stage groupings.

Survival times of $n = \Sexpr{length(tc)}$ patients from the preoperative arm 
of the CAO/\-ARO/\-AIO-94 trial \citep{Sauer2004} are under test ($48$ patients with distant
metastases found at surgery were excluded from this analysis, so all patients are classified M0). 
Therefore, we propose a new maximally selected statistic for a censored response
and two ordered covariates with potential interactions. Log-rank scores are used
as influence function $h$ for the censored response and the potential partitions
$g$ are constructed from all combinations of the five T and three N categories.
As both categories are ordered, only those partitions are used which are ordered
in T given N and vice versa yielding 194 %FIXME% can we compute this?
candidate partitions, $\Sexpr{length(stat)}$ of which meet the sample size constraints.
The maximum of the absolute values
of the corresponding \Sexpr{length(stat)} standardized statistics is 
\Sexpr{round(teststat, 3)} with a $p$-value smaller than $0.0001$. 
The partition chosen by the algorithm identifies all patients with
N category N2 or N3 as being under high risk and almost all patients
from N0 and N1 as being under low risk. As an exception, 
one single patient with ypT4 and N1 is assigned to the high risk group as
well---whether or not this decision is sensible or results from random variation
cannot be judged based on one observation alone.
Figure~\ref{fig:maxstat} depicts Kaplan-Meier estimates of the survival
times in the two risk groups.

The TNM staging system defines stages I and II by ypT1/2 and ypT3/4, respectively, 
whereas the distinction between stages I/II and III is defined via N0 vs. N1/2/3. 
Thus, the split between N1 and N2 identified by our procedure leads to a new prognostic group
within TNM stage III. One might argue that this could be explained by an improved
response of N1 patients to the new therapy \citep[see][also]{Roedel:2007} but a definitive 
statement is far beyond the scope of this paper.

\section{Discussion}

Maximally selected statistics for the estimation of simple cutpoint models have been
in use since many years. 
%%From an academic point of view, such models are almost always an
%%over-simplification. 
Many researchers appreciate a model that is easy to 
communicate and easy to implement in practical situations. 
Of course, the trade-off between 
simplicity and accuracy has to be carefully investigated.

A new class of generalized maximally selected statistics based on
the conditional inference framework of \cite{StrasserWeber1999} allows for a unified treatment 
of different kinds of maximally selected statistics. Test procedures from
this framework can be adapted to new test problems by specifying an influence
function $h$, suitable for the scale level of the response, and setting up a set
of candidate partitions $g$ determined from the available covariates. As
the number of potential partitions can become large, efficient algorithms are
required for evaluating the distribution of the maximum statistic. For
partitions based on cutpoints, we provide such an algorithm that computes
the asymptotic distribution in linear time by exploiting the special product
structure of the correlation matrix.
%% and utilizing numerical integration techniques.

The implementation of (known and newly designed) maximally selected statistics 
only requires the specification of the binary candidate partitions, via a function $g$, and
a problem-specific influence function $h$. Linear statistics $\T$ and the test statistic
$\Tmax$ can be computed in the \textsf{R} system for statistical computing \citep{PKG:R}
utilizing the function \texttt{maxstat\_test()} from the add-on package
\Rpackage{coin} \citep{Hothornetal2006,PKG:coin}.
The distribution of $\Tmax$ can be approximated by its asymptotic distribution or by Monte-Carlo methods 
(also in the presence of blocks, e.g., in multicenter trials) readily
available from the same package.

In summary, a unified treatment of maximally selected statistics 
for nominal, ordered, discrete and continuous numeric, censored and
multivariate response variables as well as nominal, ordered and 
multivariate covariates to be dichotomized is now possible 
both conceptually and practically.

%% TODO
%% wollen wir quadratische Formen mit einbauen?
%% c_\text{quad}(\T, \mu, \Sigma)  & = & (\T - \mu) \Sigma^+ (\T - \mu)^\top,
%% involving the Moore-Penrose inverse $\Sigma^+$ of $\Sigma$.
%%
%% kann man nicht eine ganz einfache approximation fuer P(T > max(Z1, ..., Zpq)) herleiten?

<<ack, echo = FALSE, results = tex>>=
if (style == "Z")
    cat("\\input{acknowledgement}\n")
@

\bibliography{maxstat}

%%TH: Acknowledgements

%%\section*{Appendix}

%%By the same arguments as used in Section~3.3 of \cite{Worsley1982}, for some $c > 0$
%%\begin{eqnarray*}
%%\Prob(\Tmax > c) = \Prob\left(\bigcup_{j = 1}^p |Z_j| > c\right) \le
%%\sum_{j = 1}^p \Prob(|Z_j| > c) - \sum_{j = 1}^{p - 1} \Prob\left(|Z_j| > c \cap |Z_{j+1}| > c\right)
%%\end{eqnarray*}
%%where computing $\Prob\left(|Z_j| > c \cap |Z_{j+1}| > c\right)$ only requires the evaluation
%%of a bivariate standard normal distribution with correlation $\rho_{j,j+1}$.

\end{document}
