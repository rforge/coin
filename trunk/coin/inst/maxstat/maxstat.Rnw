\documentclass[shortnames]{Z}

%% need no \usepackage{Sweave}
\SweaveOpts{engine=R, eps=FALSE, echo=FALSE, results=hide}

\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amsmath}
\newcommand{\V}{\mathbb{V}} %% cal{\mbox{\textnormal{Var}}} }
\renewcommand{\E}{\mathbb{E}} %%mathcal{\mbox{\textnormal{E}}} }
\newcommand{\Var}{\mathbb{V}} %%mathcal{\mbox{\textnormal{Var}}} }
\newcommand{\argmin}{\operatorname{argmin}\displaylimits}
\newcommand{\argmax}{\operatorname{argmax}\displaylimits}
\newcommand{\X}{\mathbf{X}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\T}{\mathbf{T}}  
\renewcommand{\vec}{\text{vec}}
\newcommand{\R}{\mathbb{R}}  
\newcommand{\Rb}{\mathbf{R}}  
\newcommand{\Rpackage}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\Tmax}{T_\text{max}}

\newcommand{\indic}{I}

\title{A Permutation-based View on Maximally Selected Statistics and Cutpoint Estimation}
\author{Torsten Hothorn\\Friedrich-Alexander-Universit\"at\\Erlangen-N\"urnberg
  \And Achim Zeileis\\Wirtschaftsuniversit\"at Wien}
\Plainauthor{Torsten Hothorn, Achim Zeileis}
\Keywords{conditional inference, asymptotic distribution,  
          maximally selected statistics, changepoint}

\Abstract{
Maximally selected statistics for the estimation of simple cutpoint models
are embedded into a powerful conceptual framework for conditional inference
procedures.
%Z% Both the abstract and the intro could send a clearer message, I guess...
The conditional counterparts of most of the published procedures in this area are
described. The conditional asymptotic distribution for a large class of
maximally selected statistics is given.
The new framework is used to search for a high-risk group of rectal
cancer patients treated with a neo-adjuvant chemoradiotherapy.
}

\begin{document}

<<packages>>=
rseed <- 20061103
library("xtable")
library("survival")
@

\section{Introduction} \label{sec:introduction}

Dichotomization of variables measured at higher scale levels 
prior to model building is bad practice 
\citep[][among many others]{Royston2006}.
It will result in loss of power and sophisticated regression
models that adapt itself to the complexity of the regression 
problem at hand are widely available. However, we need to acknowledge
that the implementation of scientific results into the real world, such as
clinical practice, requires simple `good--poor' or `high--low' 
decision rules.
%Z% Is this the only justification?
%Z% Capturing step-shaped relationships should be another one?

Such rules of thumb are frequently used to investigate new predictor
variables for patient survival in oncology. \cite{Galon2006} estimate
cutpoints for various characteristics of immune cells within colorectal
tumor samples, such as type, density or location, with respect to their
ability to differentiate between patients with good and poor prognosis.
\cite{Buccisano2006} follow a similar approach, obtaining a threshold for
residual leukemic cells in acute myeloid leukemia patients. The ability for
expression levels of HER2 and coamplified genes to predict breast cancer
survival is investigated by \cite{Vinatzer2005} utilizing maximally selected
log-rank statistics. Beyond applications in oncology, 
cutpoints for habitat factors discriminating between ecosystems with 
low and high abundance of certain indicator species 
are of increasing interest \citep[see][]{Huggett2005}. For example,
\cite{Muller2004} derive cutpoint-based rules of thumb for certain 
bird species and habitat factors in oak forests.
An inspection of currently published scholary articles
citing one of the statistical papers in this area 
\citep[for example][]{Mazumdar2000,HothornLausen2003} reveals many 
more applications.

Two questions arise from a statistical point of view. In a first step, we
have to make sure that a simple cutpoint model has, at least, some meaning and
%Z% I know what you want to say...however, readers might misunderstand
%Z% this: We have no built-in check that either the null or alternative
%Z% hypothesis actually fits the data.
in a second step we want to estimate the `best' cutpoint. It is convenient
to deal with both problems separately. The first problem needs to be addressed by
a formal hypothesis test for the null hypothesis of independence between covariate 
(to be dichotomized)
and response variable. A test with power against shift alternatives, i.e.,
departures from the null hypothesis where the distribution of the response variable 
varies between two groups of observations, is of special interest. 
Once we are able to reject the null hypothesis, we are interested in the 
alternative which lead to the rejection, i.e., want to estimate a `cutpoint'.

The first procedure of this kind, utilizing the maximum over multiple $\chi^2$ statistics
for $2 \times 2$ tables, was described by \cite{MillerSiegmund1982}. 
\cite{LausenSchumacher1992} derived an approximation for the unconditional asymptotical
distribution of maximally selected rank statistics, extending the area of application
to continuous and censored response variables. \cite{Betensky1999} propose
a maximally selected $\chi^2$ test for nominal response variables measured at
$k > 2$ levels and ordered categorical data (maximally selected Cochran-Armitage test).
Finally, \cite{Rabinowitz2000} suggested a maximally selected McNemar's test.

In the last years, improvements for the approximation of the null distribution
of maximally selected statistics where published by \cite{HothornLausen2003} and
\cite{Boulesteix2006a,Boulesteix2006b}. \cite{Lausenetal2004} extended maximally
selected rank statistics to more than one covariate.

We embed the different statistics known so far into a common framework and take
a conditional view on maximally selected statistics. Conditioning on permutations
of the data allows for a unified treatment of maximally selected statistics
within the theory of permutation tests described by \cite{StrasserWeber1999}.
The complete correlation structure of all statistics can be computed efficiently and 
the conditional asymptotic distribution for a large class of maximally selected 
statistics can be derived from Theorem 3 of \cite{StrasserWeber1999}. 
Since no assumptions on the scale level of neither the covariate to be dichotomized nor
the response variable are necessary, the concept can be used to implement
already published procedures and to extend the methodology to new areas of 
application.

\section{Binary partitions and two-sample statistics} \label{sec:stat}

We are provided with independent and identically distributed
observations $(\Y_i, \X_i)$ for $i = 1, \dots, n$ and 
are interested in testing the null hypothesis of independence of the response 
variable $\Y$ and and covariate(s) $\X$
\begin{eqnarray*}
H_0: D(\Y | \X) = D(\Y)
\end{eqnarray*}
against shift alternatives. That is, departures from the null hypothesis
where the distribution of the response variable varies between two groups
of observations are of special interest.
Such binary partitions are defined in advance by $p$ sets $A_1, \dots, A_p$
partitioning the observations into two groups based on the $\X$ measurements only. Here,
\begin{eqnarray*}
g_j(\X_i) \quad = \quad \indic(\X_i \in A_j) \qquad (i = 1, \dots n)
\end{eqnarray*}
%Z% highlight special case of cutpoints?
denotes the indicator function partitioning the observations 1, \dots, n into two groups. 
Only binary partitions satisfying the sample size 
constraint $\sum_i g_i(\X_i) \in (n\varepsilon, n - n\varepsilon)$ 
for some fixed $\varepsilon \in (0, 0.5)$ are taken into account. 

The two-sample problem associated with the $j$th binary partition
can be tested using a linear statistic
\begin{eqnarray*}
\T_j \quad = \quad \vec\left(\sum_{i = 1}^n g_j(\X_i) h(\Y_i)^\top\right) \in \R^{q \times 1}
\end{eqnarray*}
where $h: \mathcal{Y} \rightarrow
\R^{q \times 1}$ is an \emph{influence function} applied to the responses.
The function $h(\Y_i) = h(\Y_i, (\Y_1, \dots, \Y_n))$ may depend on the full 
vector of responses $(\Y_1, \dots, \Y_n)$, however only
in a permutation symmetric way, i.e., the value of the
function must not depend on the order in which $\Y_1, \dots, \Y_n$ appear.
For example, with $h$ being a rank transformation for a 
continuous response $\Y$, the linear
statistic is the sum of the ranks for observations $i$ with $g_j(\X_i) = 1$, 
i.e., equals the Wilcoxon-Mann-Whitney statistic. 

A joint linear statistic for all binary partitions is
\begin{eqnarray*}
\T \quad = \quad (\T_1, \dots, \T_p) = \vec\left(\sum_{i = 1}^n g(\X_i) h(\Y_i)^\top\right)
\in \R^{pq \times 1}
\end{eqnarray*}
including all $p$ two-sample partitions $g(\X_i) = (g_1(\X_i), \dots, g_p(\X_i))$ 
simultaneously for testing $H_0$.

\section{Standardization, inference and estimation} \label{sec:inf}

The distribution of $\T$  depends on the joint distribution of $\Y$ and $\X$,
which is unknown under almost all practical circumstances. 
At least under the null hypothesis one can dispose of this 
dependency by fixing $\X_1, \dots, \X_n$ and conditioning on all possible
permutations $S$ of the responses $\Y_1, \dots, \Y_n$. Tests that have been
constructed by means of this conditioning principle are called 
\emph{permutation tests} and we adopt this conditional view for inference and estimation
in the following.
%Z% Should we try to make the dependency on our previous work
%Z% a little less obvious?

The conditional expectation $\mu \in \R^{pq \times 1}$ and covariance
$\Sigma \in \R^{pq \times pq}$ of $\T$ under $H_0$ given
all permutations $\sigma \in S$ of the responses are derived by
\cite{StrasserWeber1999}:
\begin{eqnarray*}
\mu = \E(\T | S) & = & \vec \left( \left( \sum_{i = 1}^n g(\X_i) \right)
\E(h | S)^\top \right) \\
\Sigma = \V(\T | S) & = &
    \frac{n}{n - 1}  \V(h | S) \otimes
        \left(\sum_i g(\X_i) \otimes  g(\X_i)^\top \right)
\\
& - & \frac{1}{n - 1}  \V(h | S)  \otimes \left(
        \sum_i g(\X_i) \right) \otimes \left( \sum_i g(\X_i)\right)^\top
\nonumber
\end{eqnarray*}
where $\otimes$ denotes the Kronecker product, and the conditional
expectation of the influence function is $\E(h | S) = n^{-1} \sum_i
h(\Y_i)$ with corresponding $q \times q$ covariance matrix
\begin{eqnarray*}
\V(h | S) = n^{-1} \sum_i \left(h(\Y_i) - \E(h | S) \right) \left(h(\Y_i) - \E(h | S)\right)^\top.
\end{eqnarray*}

The key step for the construction of a maximally selected 
statistic based on the multivariate
linear statistic $\T$ is its standardization utilizing the
conditional expectation $\mu$ and the diagonal elements of the 
covariance matrix $\Sigma$: the 
maximum of the absolute values of the standardized linear statistic
is used as test statistic
\begin{eqnarray*}
\Tmax  & = & \max \left| \frac{\T - \mu}{\text{diag}(\Sigma)^{1/2}} \right|.
\end{eqnarray*}

The exact conditional distribution of $\Tmax$ under $H_0$ can be approximated by 
Monte Carlo methods (randomly draw a number of permutations of $1, \dots, n$ 
and evaluate the test statistic $\Tmax$ for shuffled responses $\Y$)
or by its limiting distribution. For $n \rightarrow \infty$ the
distribution of the multivariate linear statistic $\T$ tends to a multivariate 
normal distribution with mean $\mu$ and covariance matrix $\Sigma$ \citep[Theorem 3][]{StrasserWeber1999}.
Thus, we have to evaluate $\Prob(\Tmax > c) = \Prob(\max(|Z_1|, ..., |Z_{pq}| > c)$ for standard normal
random variables $Z_1, \dots, Z_{pq}$ with correlation matrix $\text{cor}(\Sigma)$ and some $c > 0$. In fact,
this conditional asymptotical distribution coincides with the unconditional asymptotical
distribution obtained for maximally selected rank statistics \citep[e.g.][]{HothornLausen2003}.
When the dimensionality $pq$ becomes too large to evaluate the 
the corresponding $pq$-dimensional multivariate normal distribution ($pq > 100$, say), 
the inequality presented by \cite{Worsley1982} provides us with an upper bound
for $\Prob(\Tmax > c)$ as $n \rightarrow \infty$ 
(see Appendix).

When the test statistic is large enough to indicate a deviation from the null hypothesis, we
are interested in determining the partition with largest standardized statistic:
the best separating partition is $A_{j^\star}$ with 
\begin{eqnarray*}
j^\star = \argmax_j \left|\T_j - \mu_j\right| \times \Sigma_{j,j}^{-1/2}.
\end{eqnarray*}
%Z% Either use the same layout as above or simply refer to the argmax
%Z% in the formula above.

\section{Applications and illustration}

Maximally selected statistics as described in Section~\ref{sec:stat} can be
applied to covariates $\X$ and responses $\Y$ measured at arbitrary scales; appropriate 
influence functions $h$ for nominal, ordered, numeric, censored and multivariate response
variables are given in the sequel, preceded by a description of how to
partition the covariate space for nominal, ordered and multivariate covariates.

The most important situation of an univariate and at least ordinally measured covariate $\X$ leads
to partitions induced by cutpoints defined by the realizations $\X_1, \dots, \X_n$. More specifically,
$A_j = (-\infty, \xi_j]$, where $\xi_j$ is the $j$th element of the increasingly sorted unique 
realizations of $\X$ (the sample size constraints apply, of course). Thus, having
identified the best separating partition $A_{j^\star}$, the estimated cutpoint is
$\xi_{j^\star}$. For nominal covariates, all $2^{k-1}$ binary partitions of the $k$ levels 
that meet the sample size constraints are taken into account.
For multiple covariates, we simply look at all binary partitions induced by all covariates
simultaneously. This has been suggested for the special case of maximally selected rank statistics
by \cite{Lausenetal2004}.

The influence function $h$ has to be chosen depending on the scale level of the 
response variable $\Y$. For nominal responses, $h$ is simply a dummy coding of the 
$k$ factor levels, i.e., $h(\Y_i)$ is the unit vector of length $k$. 
It is sufficient to code for the first $k - 1$ levels only. For binary responses,
the statistic $\Tmax$ is equivalent to a maximally selected $\chi^2$ statistic 
\citep{MillerSiegmund1982,Koziol1991}. For nominal responses with $k > 2$ levels,
the $\Tmax$ statistic is the maximum over the maximum of 
standardized $k \times 2$ tables, an alternative to maximally selected $\chi^2$ statistics
for larger tables \citep{Betensky1999}. 

An ordinal response $\Y$ requires the specification of scores attached to each of the $k$ levels and 
$h$ is the vector of those numeric scores. For equidistant scores $1, \dots, k$, the statistic $\Tmax$ 
is equivalent to a maximally selected Cochran-Armitage statistic \citep{Betensky1999}, however, 
no restrictions to the choice of scores apply here.

Many possible influence functions are available for discrete or continuous covariates;
the identity transformation is the most natural choice, rank transformations are possible as well. 
For the latter, the test statistic $\Tmax$ corresponds to maximally selected rank statistics 
\citep{LausenSchumacher1992}.  When a numeric response is censored, log-rank transformations or
Savage scores can be applied.

For a multivariate response, possibly consisting of variables with different scale levels,
the influence function $h$ is a combination of 
influence functions appropriate for any of the univariate response variables
as suggested below. In the presence of a grouping of the observations
into independent blocks, only permutations within blocks are eligible and the
conditional expectation and covariance matrix need to be computed
separately for each block. Therefore, it is easily possible to take a block randomization
scheme in a randomized clinical trial into account. In the extreme case of each observation
being a block in repeated measurements, $\Tmax$ corresponds to maximally selected McNemar's statistics
for binary responses \citep{Rabinowitz2000}.

<<cao-data>>=
<<tntab, echo = FALSE, results = tex>>=
load("example/preOP_maxstat.rda")
tc <- preOP$tclass
nc <- preOP$nclass
levels(tc) <- gsub("p", "yp", levels(tc))
levels(nc) <- gsub("p", "", levels(nc))
tab <- xtabs(~ tc + nc, data = preOP)
names(dimnames(tab)) <- c("T category", "N category")
load("example/maxstat.rda")
risk <- preOP$tn %in% cutpoint
@

As an illustration, we attempt to identify patients with poor prognosis among 
patients suffering from rectal 
cancer treated with a neo-adjuvant chemoradiotherapy regime. Here, we
search for a combination of pathological T and N categories that can help to 
differentiate between two risk groups.
Survival times of $n = \Sexpr{length(tc)}$ patients from the preoperative arm 
of the CAO/ARO/AIO-94 trial \citep{Sauer2004} are under test ($48$ patients with distant
metastases found at surgery were excluded from this analysis). 
The response variable is censored, so we choose a log-rank transformation as influence function $h$.
From the potentially $\Sexpr{2^(nlevels(preOP$tn)-1)}$ partitions,
$\Sexpr{length(stat)}$ meet the sample size constraints. The maximum of the absolute values
of the corresponding \Sexpr{length(stat)} standardized statistics is \Sexpr{round(teststat, 3)}. The null
hypothesis can be rejected. All patients with N category N2 or N3 are under high risk; in addition,
one patient %Z% One should probably add some remark about this...
with ypT4 and N1 is under high-risk as well. Figure~\ref{fig:maxstat} depicts Kaplan-Meier 
estimates of the survival times in the two risk groups.


\begin{table}[t]
\begin{center}
\caption{Pathological T and N category of $351$ rectal cancer patients 
         treated with a preoperative chemoradiotherapy. \label{tab:tn}}
<<tntab, echo = FALSE, results = tex>>=
tab <- cbind(tab, rowSums(tab))
tab <- rbind(tab, colSums(tab))
cat("\\begin{tabular}{|r|rrr|r|} \\hline \n")
cat(" & \\multicolumn{3}{|c|}{N category} & \\\\ \n")
cat("T category & ", paste(levels(nc), collapse = " & "), " & \\\\ \\hline ")
for (i in levels(tc))
cat(i, " & ", paste(tab[i,], collapse = " & "), "\\\\", ifelse(i=="ypT4", "\\hline", ""), "\n")
cat(" & ", paste(tab[nrow(tab),], collapse = " & "), "\\\\ \\hline", "\n")
cat("\\end{tabular} \n")
@
\end{center}
\end{table}

\setkeys{Gin}{width = 0.75\textwidth}
\begin{figure}[h!]
\begin{center}
\caption{%FIXME: omitted?% Absolute standardized test statistics (left panel) and 
Survival times of rectum cancer patients in two risk groups identified by a maximally selected log-rank
statistic. \label{fig:maxstat}}
<<maxstat, echo = FALSE, fig = TRUE, width = 7, height = 6>>=
###layout(matrix(1:2, ncol = 2))
###plot(abs(stat), xlab = "Index", ylab = "T")
plot(survfit(Surv(time, event) ~ risk, data = preOP), xlab = "Time (in months)",
     ylab = "Overall Survival Probability")
text(85, 0.92, paste("N0 or (N1 but not ypT4); n =",table(risk)[2]))
text(85, 0.30, paste("N2 + N3 or (N1 and ypT4); n =", table(risk)[1]))
@
\end{center}
\end{figure}


\section{Discussion}

Maximally selected statistics for the estimation of simple cutpoint models have been
in use since many years. From an academic point of view, such models are almost always an
over-simplification. However, many researchers appreciate a model that is easy to 
communicate and easy to implement in practical situations. The tradeoff between 
between simplicity and accuracy has to be carefully investigated.

The conceptual framework by \cite{StrasserWeber1999} allows for a unified treatment 
of different kinds of maximally selected statistics. When we are faced with a new problem, 
only the influence function $h$ needs to be newly designed. The complete correlation 
structure is then available from the covariance matrix $\Sigma$. Moreover, the asymptotic
distribution of the maximum statistic is known as well.

The implementation of (known and newly designed) maximally selected statistics 
only requires the specification of the binary partitions, via a function $g$, and
a problem-specific influence function $h$. Linear statistics $\T$ and the test statistic
$\Tmax$ can be computed utilizing the \texttt{maxstat\_test()} function in
\textsf{R} \citep{PKG:R} in the add-on package \Rpackage{coin} \citep{Hothornetal2006,PKG:coin}.
The distribution
of $\Tmax$ can be approximated by its asymptotic distribution or by Monte-Carlo methods 
(also in the presence of a grouping of the observations into independent blocks) readily
available from the same package.

In summary, a unified treatment of maximally selected rank statistics 
for nominal, ordered, discrete and continuous numeric, censored and
multivariate response variables as well as nominal, ordered and 
multivariate covariates to be dichotomized is now possible conceptually and practically.

%% TODO
%% wollen wir quadratische Formen mit einbauen?
%%c_\text{quad}(\T, \mu, \Sigma)  & = & (\T - \mu) \Sigma^+ (\T - \mu)^\top,
%%involving the Moore-Penrose inverse $\Sigma^+$ of $\Sigma$.
%%
%% kann man nicht eine ganz einfache approximation fuer P(T > max(Z1, ..., Zpq)) herleiten?

\section*{Acknowledgements}

The work of T. Hothorn was supported by Deutsche Forschungsgemeinschaft (DFG) under grant HO 3242/1-3.

\bibliography{maxstat}

\section*{Appendix}

Let $A_j = (-\infty, \xi_j]$ with $\xi_j < \xi_k$ for $1 \le j < k \le p$ and $q = 1$. Then,
the correlation between $\T_j$ and $\T_k$ is given by
\begin{eqnarray*}
\rho_{j,k} = \frac{\Sigma_{j,k}}{\sqrt{\Sigma_{j,j} \Sigma_{k,k}}} = 
\sqrt{\frac{\left(n - \sum_i g_k(\X_i)\right) \sum_i g_j(\X_i)}
                        {\left(n - \sum_i g_j(\X_i)\right) \sum_i g_k(\X_i)}}.
\end{eqnarray*}
By the same arguments as used in Section~3.3 of \cite{Worsley1982}, for some $c > 0$
\begin{eqnarray*}
\Prob(\Tmax > c) = \Prob\left(\bigcup_{j = 1}^p |Z_j| > c\right) \le
\sum_{j = 1}^p \Prob(|Z_j| > c) - \sum_{j = 1}^{p - 1} \Prob\left(|Z_j| > c \cap |Z_{j+1}| > c\right)
\end{eqnarray*}
where computing $\Prob\left(|Z_j| > c \cap |Z_{j+1}| > c\right)$ only requires the evaluation
of a bivariate standard normal distribution with correlation $\rho_{j,j+1}$.

We have to evaluate the probability that any of $|Z_1|, \dots, |Z_p|$ exceeds
$c > 0$:
\begin{eqnarray*}
\Prob(\Tmax > c) & = & 1 - 
\frac{1}{\sqrt{|\Rb| (2\pi)^p}} \int\limits_{-c}^{c} 
\exp\left(-\frac{1}{2}\z^\top \Rb^{-1}\z \right) d\z.
\end{eqnarray*}
The inverse of the covariance matrix $\Rb = \text{cor}(\Sigma)$ is tridiagonal
and symmetric, i.e.,
\begin{eqnarray*}
\Rb^{-1} = \left( \begin{array}{cccccc}
r_{1,1} & r_{1,2} & 0       & 0 & \dots & 0 \\
r_{1,2} & r_{2,2} & r_{2,3} & 0 & \dots & 0 \\
      0 & r_{2,3} & r_{3,3} & r_{3,4} & \dots & 0  \\
      0 & 0 & r_{3,4} & r_{4,4} & \dots & 0  \\
\vdots & \vdots   &  \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & 0 & r_{p-1,p-1}& r_{p-1,p} \\
0 & 0 & 0 & 0 & 0 & r_{p,p} 
\end{array}
\right)
\end{eqnarray*}
Thus, the quadratic form $\z^\top \Rb^{-1}\z$ simplifies to
\begin{eqnarray*}
\z^\top\Rb^{-1}\z = r_{1,1}z_1^2 + 2r_{2,1}z_1z_2 + r_{2,2} z_2^2 + \dots +
2r_{p,p-1} z_pz_{p-1} + r_{p,p}z_p^2.
\end{eqnarray*}
With $\phi(z) = \exp\left(-z/2\right)$ we have
\begin{eqnarray*}
& & \frac{1}{\sqrt{|\Rb| (2\pi)^p}} \int\limits_{-c}^{c} 
\phi\left(\z^\top \Rb^{-1}\z \right) d\z = \\
& & \int\limits_{-c}^c \phi(r_{1,1}z_1^2)
\int\limits_{-c}^c \phi(2r_{2,1}z_1z_2 + r_{2,2} z_2^2)
\int\limits_{-c}^c \dots
\int\limits_{-c}^c \phi(2r_{p,p-1} z_pz_{p-1} + r_{p,p}z_p^2) d \z
\end{eqnarray*}
Now with recursively defined functions $f_j$ ($j = 2, \dots, p + 1$)
\begin{eqnarray*}
f_j(z) & = & \int\limits_{-c}^c \phi\left(2 r_{j, j-1} z \tilde{z} + r_{j,j} \tilde{z}^2\right)
             f_{j+1}(\tilde{z}) d\tilde{z} 
            \quad \forall j = 2, \dots, p \\
f_{p+1}(z) & = & 1
\end{eqnarray*}
the above integral can be fomulated recursively:
\begin{eqnarray*}
\frac{1}{\sqrt{|\Rb| (2\pi)^p}} \int\limits_{-c}^{c} 
\phi\left(\z^\top \Rb^{-1}\z \right) d\z = 
\int\limits_{-c}^c \phi(r_{1,1}z^2) f_2(z) dz.
\end{eqnarray*}
This integral can be approximated numerically in $O(p)$ starting with $f_p$. For a grid
over $z \in [-c, c]$ and $\tilde{z} \in [-c,c]$ values, the function $f_j$ is
evaluated and aggregated over $\tilde{z}$ only, yielding values
of $f_j(z)$ over a grid of $z$ values. For $f_{j-1}$, this grid is simply reused.

\end{document}
